{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "네이버크롤링_Kiwi_23",
      "provenance": [],
      "collapsed_sections": [
        "6qW1Lg8vNfM-",
        "Hs8x2Y7yMT_l",
        "AW9C899D02Lc",
        "1c7JNaAmGBGX",
        "VjL1xWakIxX9",
        "NBAueqUoSSl7",
        "PKJzz5Lmem8b"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Minsung-commit/TIL/blob/master/%EB%84%A4%EC%9D%B4%EB%B2%84%ED%81%AC%EB%A1%A4%EB%A7%81_Kiwi_23.1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2D3n6CcKAug"
      },
      "source": [
        "%%bash\n",
        "apt-get update\n",
        "apt-get install g++ openjdk-8-jdk python-dev python3-dev\n",
        "pip3 install JPype1\n",
        "pip3 install konlpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "817hwtGGKHHV",
        "outputId": "3836cac8-6a73-40e3-cd70-dbe71dc36d76"
      },
      "source": [
        "%env JAVA_HOME \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: JAVA_HOME=\"/usr/lib/jvm/java-8-openjdk-amd64\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo2knITZKHhs"
      },
      "source": [
        "%%bash\n",
        "bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
        "pip3 install /tmp/mecab-python-0.996"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDVQxjTbKI-K"
      },
      "source": [
        "import konlpy\n",
        "from konlpy.tag import Kkma, Komoran, Hannanum, Okt\n",
        "from konlpy.utils import pprint\n",
        "from konlpy.tag import Mecab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        },
        "id": "OtihWGQWeDEP",
        "outputId": "3add44c5-4ef0-4af4-d834-e9b8a2b2388d"
      },
      "source": [
        "pip install pyLDAvis"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyLDAvis\n",
            "  Downloading pyLDAvis-3.3.1.tar.gz (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 5.4 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.0.1)\n",
            "Collecting numpy>=1.20.0\n",
            "  Downloading numpy-1.21.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 195 kB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.4.1)\n",
            "Collecting funcy\n",
            "  Downloading funcy-1.16-py2.py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.11.3)\n",
            "Collecting pandas>=1.2.0\n",
            "  Downloading pandas-1.3.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.3 MB 58.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (57.4.0)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (3.6.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.22.2.post1)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.7.3)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyLDAvis) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->pyLDAvis) (5.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->pyLDAvis) (2.0.1)\n",
            "Building wheels for collected packages: pyLDAvis\n",
            "  Building wheel for pyLDAvis (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyLDAvis: filename=pyLDAvis-3.3.1-py2.py3-none-any.whl size=136897 sha256=6221f64987ce2b362c83b0160c8c13ee5257fe979e566c6dea1e76b69c06ae38\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/21/f6/17bcf2667e8a68532ba2fbf6d5c72fdf4c7f7d9abfa4852d2f\n",
            "Successfully built pyLDAvis\n",
            "Installing collected packages: numpy, pandas, funcy, pyLDAvis\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.1.5\n",
            "    Uninstalling pandas-1.1.5:\n",
            "      Successfully uninstalled pandas-1.1.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.6.0 requires numpy~=1.19.2, but you have numpy 1.21.2 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 1.3.3 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed funcy-1.16 numpy-1.21.2 pandas-1.3.3 pyLDAvis-3.3.1\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "pandas"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7HFuujt9pKT"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import time"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTVw7pKMNagJ"
      },
      "source": [
        "# OKT, Mecab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qW1Lg8vNfM-"
      },
      "source": [
        "## 데이터 로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEqeF2ku9eHd"
      },
      "source": [
        "data = pd.read_csv('./naver_blog_crawling.csv') #데이터셋 로드\n",
        "data = data[['title','content']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "id": "4XvUbq9lW8R8",
        "outputId": "b657de50-4b02-4330-d22e-7f9e749cdbee"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_repr_html_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m         \"\"\"\n\u001b[1;32m    800\u001b[0m         \u001b[0mReturn\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtuple\u001b[0m \u001b[0mrepresenting\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdimensionality\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_html\u001b[0;34m(self, buf, encoding, classes, notebook, border)\u001b[0m\n\u001b[1;32m    986\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mlist\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mtuple\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 988\u001b[0;31m         \u001b[0mnotebook\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    989\u001b[0m         \u001b[0mborder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m         \u001b[0mtable_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NotebookFormatter' object has no attribute 'get_result'"
          ]
        },
        {
          "data": {
            "text/plain": [
              "                                              title                                            content\n",
              "0                             제주도 감성숙소 에어비앤비로 다녀왔어요  안녕하세요, 오랜만에 여행 포스팅을 하러 왔습니다! 두둥:) 다녀온지는 좀 되었는데...\n",
              "1                            아고다 9월 할인코드 감성숙소 서울 호텔  얼마전에 다녀온 서울 호텔 후기입니다. 휴가를 잘 낼 수 없는 상황이라 틈날때마다 ...\n",
              "2                                제주 동쪽 구좌 감성숙소 삼일삼오  이번 제주 여행에서 가장 신경썼던건 바로 숙소를 고르는 일 아무래도 여행의 주제 자...\n",
              "3                           청주 메이킷슬로우 에어비앤비 감성숙소 후기  오늘은 제가 지난 휴가때 찐친들과 다녀온  청주 메이킷슬로 라는 에어비앤비 감성숙소...\n",
              "4                           인천 한옥 독채 펜션, 감성숙소 '시우재'  지인의 소개로 알게된 #시우재 마침 이것저것 기념일도 있고 해서 시우재 알게된 날 ...\n",
              "...                                             ...                                                ...\n",
              "3937       #168. 양양 낙산비치호텔, 오션뷰/마운틴뷰 감성 숙소 (+예약 링크)  여행갈때 거의 에어비앤비, 감성숙소를 찾아다니는 나도  부모님이랑 여행갈때나  마땅...\n",
              "3938                  강릉 감성숙소 언제나 소돌 뷰 맛집 204호 솔직후기  강릉에는 예쁜 감성숙소가 많기로 유명하잖아요 !! 특히 강릉에는 뷰가 예쁜 숙소가 ...\n",
              "3939                     [숙소] 내가 가고 싶은 제주도 감성 숙소 2탄  이전에 썻던 제주도 감성 숙소 1탄에 이어  2탄을 포스팅 했어요~  저번에는 10...\n",
              "3940  [제주일기]여심저격! 촬영 스튜디오 같은 제주도 모슬포 감성숙소 '아빈스인모슬포'  #제주도숙소추천 #제주도감성숙소 #제주도인스타숙소 #아빈스인모슬포 #서귀포숙소 #서...\n",
              "3941                           전주근교 감성숙소펜션 완주 '여림재'  안녕하세요:) 주말만 기다리며 뭐하고 놀까, 힐링하러 가야겠다 싶어서 선택한 완주에...\n",
              "\n",
              "[3942 rows x 2 columns]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "tsEiyQPErvhi",
        "outputId": "f927c147-eb05-446a-c69b-c0c80a126dbc"
      },
      "source": [
        "data.dropna(axis=0, inplace=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-1edeb318a43a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdropna\u001b[0;34m(self, axis, how, thresh, subset, inplace)\u001b[0m\n\u001b[1;32m   5010\u001b[0m         \u001b[0mx\u001b[0m  \u001b[0;36m1\u001b[0m  \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5011\u001b[0m         \u001b[0my\u001b[0m  \u001b[0;36m2\u001b[0m  \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5012\u001b[0;31m         \u001b[0mz\u001b[0m  \u001b[0;36m3\u001b[0m  \u001b[0;36m6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5014\u001b[0m         \u001b[0mCast\u001b[0m \u001b[0mindex\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0mto\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdifferent\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/ops/common.py\u001b[0m in \u001b[0;36mnew_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     63\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/ops/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0;31m#  to avoid constructing two potentially large/sparse DataFrames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m     join_columns, _, _ = left.columns.join(\n\u001b[0;32m--> 370\u001b[0;31m         \u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"outer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_indexers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m     )\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0mrvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_wrapped_if_datetimelike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     \u001b[0mrvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;31m# TODO: same for tuples?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36mna_arithmetic_op\u001b[0;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_na_arithmetic_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_cmp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \"\"\"\n\u001b[0;32m--> 140\u001b[0;31m     \u001b[0mReturn\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mresult\u001b[0m \u001b[0mof\u001b[0m \u001b[0mevaluating\u001b[0m \u001b[0mop\u001b[0m \u001b[0mon\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0mIf\u001b[0m \u001b[0mnative\u001b[0m \u001b[0mtypes\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcompatible\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mtry\u001b[0m \u001b[0mcoercion\u001b[0m \u001b[0mto\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/computation/expressions.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFuncType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomputation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNUMEXPR_INSTALLED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mroperator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/computation/check.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mne\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"numexpr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"warn\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mNUMEXPR_INSTALLED\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mne\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mNUMEXPR_INSTALLED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: import_optional_dependency() got an unexpected keyword argument 'errors'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YTguE7ePhtU"
      },
      "source": [
        "# data = data.drop([data.index[1905], data.index[2101]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XW3La0fQ-xRm"
      },
      "source": [
        "#mecab 불러오기\n",
        "mecab = Mecab()\n",
        "okt = Okt()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYqqDUK1_uUY"
      },
      "source": [
        "sample = pd.read_csv('naver_blog_crawling.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "id": "f4VM2aFR_y1T",
        "outputId": "146e7369-554a-4fe1-8204-269434e8359c"
      },
      "source": [
        "sample"
      ],
      "execution_count": null,
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_repr_html_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m         \"\"\"\n\u001b[1;32m    800\u001b[0m         \u001b[0mReturn\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtuple\u001b[0m \u001b[0mrepresenting\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdimensionality\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_html\u001b[0;34m(self, buf, encoding, classes, notebook, border)\u001b[0m\n\u001b[1;32m    986\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mlist\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mtuple\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 988\u001b[0;31m         \u001b[0mnotebook\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    989\u001b[0m         \u001b[0mborder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m         \u001b[0mtable_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NotebookFormatter' object has no attribute 'get_result'"
          ]
        },
        {
          "data": {
            "text/plain": [
              "                                              title  ...                                            content\n",
              "0                             제주도 감성숙소 에어비앤비로 다녀왔어요  ...  안녕하세요, 오랜만에 여행 포스팅을 하러 왔습니다! 두둥:) 다녀온지는 좀 되었는데...\n",
              "1                            아고다 9월 할인코드 감성숙소 서울 호텔  ...  얼마전에 다녀온 서울 호텔 후기입니다. 휴가를 잘 낼 수 없는 상황이라 틈날때마다 ...\n",
              "2                                제주 동쪽 구좌 감성숙소 삼일삼오  ...  이번 제주 여행에서 가장 신경썼던건 바로 숙소를 고르는 일 아무래도 여행의 주제 자...\n",
              "3                           청주 메이킷슬로우 에어비앤비 감성숙소 후기  ...  오늘은 제가 지난 휴가때 찐친들과 다녀온  청주 메이킷슬로 라는 에어비앤비 감성숙소...\n",
              "4                           인천 한옥 독채 펜션, 감성숙소 '시우재'  ...  지인의 소개로 알게된 #시우재 마침 이것저것 기념일도 있고 해서 시우재 알게된 날 ...\n",
              "...                                             ...  ...                                                ...\n",
              "3937       #168. 양양 낙산비치호텔, 오션뷰/마운틴뷰 감성 숙소 (+예약 링크)  ...  여행갈때 거의 에어비앤비, 감성숙소를 찾아다니는 나도  부모님이랑 여행갈때나  마땅...\n",
              "3938                  강릉 감성숙소 언제나 소돌 뷰 맛집 204호 솔직후기  ...  강릉에는 예쁜 감성숙소가 많기로 유명하잖아요 !! 특히 강릉에는 뷰가 예쁜 숙소가 ...\n",
              "3939                     [숙소] 내가 가고 싶은 제주도 감성 숙소 2탄  ...  이전에 썻던 제주도 감성 숙소 1탄에 이어  2탄을 포스팅 했어요~  저번에는 10...\n",
              "3940  [제주일기]여심저격! 촬영 스튜디오 같은 제주도 모슬포 감성숙소 '아빈스인모슬포'  ...  #제주도숙소추천 #제주도감성숙소 #제주도인스타숙소 #아빈스인모슬포 #서귀포숙소 #서...\n",
              "3941                           전주근교 감성숙소펜션 완주 '여림재'  ...  안녕하세요:) 주말만 기다리며 뭐하고 놀까, 힐링하러 가야겠다 싶어서 선택한 완주에...\n",
              "\n",
              "[3942 rows x 4 columns]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5xnOqcV-LG6"
      },
      "source": [
        "## 불용어 리스트 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fs4nmArRDxvd"
      },
      "source": [
        "한국어 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgO9xFsarWMw"
      },
      "source": [
        "stop_words = '의 가 이 은 들 는 좀 잘 속초 걍 과 했 거 해서 게 찍 느낌 많이 듯 뷰 박 링크 인스타그램 할인 성수기 홈페이지 주말 블로그 인원 추가 ㅎㅎ 너무 게스트 넘 하우스 드리다 이용 위치 쓰다 진짜 넘 찍 거 먹 ㅠㅠ ㅎㅎㅎ ㅠ 물 였 ㅠㅠㅠ ㅠㅠ ㅠ ㅋㅋㅋ ㅋㅋ ㅋ ㅎㅎㅎ ㅎㅎ ㅎ 화장실 도 뭐 오픈 최대 준비 룸 빵 거 많이 방법 달리 스럽다 특별자치도 를 에어비 으로 자 에 와 한 하다 아 휴 아이구 포스팅 아이쿠 아이고 어 나 우리 저희 따라 의해 을 를 에 의 가 으로 로 에게 뿐이다 의거하여 근거하여 입각하여 기준으로 예하면 예를 들면 예를 들자면 저 소인 소생 저희 지말고 하지마 하지마라 다른 물론 또한 그리고 비길수 없다 해서는 안된다 뿐만 아니라 만이 아니다 만은 아니다 막론하고 관계없이 그치지 않다 그러나 그런데 하지만 든간에 논하지 않다 따지지 않다 설사 비록 더라도 아니면 만 못하다 하는 편이 낫다 불문하고 향하여 향해서 향하다 쪽으로 틈타 이용하여 타다 오르다 제외하고 이 외에 이 밖에 하여야 비로소 한다면 몰라도 외에도 이곳 여기 부터 기점으로 따라서 할 생각이다 하려고하다 이리하여 그리하여 그렇게 함으로써 하지만 일때 할때 앞에서 중에서 보는데서 으로써 로써 까지 해야한다 일것이다 반드시 할줄알다 할수있다 할수있어 임에 틀림없다 한다면 등 등등 제 겨우 단지 다만 할뿐 딩동 댕그 대해서 대하여 대하면 훨씬 얼마나 얼마만큼 얼마큼 남짓 이제 분 도움 여 ㅁ ㅎ ㄶ 얼마간 둥 오랜만 약간 체크 체크아웃 가격 정보 비 수기 평일 기준 약 전국  예약 되어다 스마트 빔 블루투스 스피커 이 외 드라이어 다리미 구비 되어다 있다 주방 음식 조리 가능하다 환기 제한 전남 제부도 있다 냄새 나 요리 삼가다 경주시 부탁드리다 후 번길 문의 다 가격 수기 비성수기 주소  좀 조금 다수 몇 얼마 지만 하물며 또한 그러나 그렇지만 하지만 이외에도 대해 말하자면 뿐이다 다음에 반대로 반대로 말하자면 이와 반대로 바꾸어서 말하면 바꾸어서 한다면 만약 그렇지않으면 까악 툭 딱 삐걱거리다 보드득 비걱거리다 꽈당 응당 해야한다 에 가서 각 각각 여러분 각종 각자 제각기 하도록하다 와 과 그러므로 그래서 고로 한 까닭에 하기 때문에 거니와 이지만 대하여 관하여 관한 과연 실로 아니나다를가 생각한대로 진짜로 한적이있다 하곤하였다 하 하하 허허 아하 거바 와 오 왜 어째서 무엇때문에 어찌 하겠는가 무슨 어디 어느곳 더군다나 하물며 더욱이는 어느때 언제 야 이봐 어이 여보시오 흐흐 흥 휴 헉헉 헐떡헐떡 영차 여차 어기여차 끙끙 아야 앗 아야 콸콸 졸졸 좍좍 뚝뚝 주룩주룩 솨 우르르 그래도 또 그리고 바꾸어말하면 바꾸어말하자면 혹은 혹시 답다 및 그에 따르는 때가 되어 즉 지든지 설령 가령 하더라도 할지라도 일지라도 지든지 몇 거의 하마터면 인젠 이젠 된바에야 된이상 만큼 어찌됏든 그위에 게다가 점에서 보아 비추어 보아 고려하면 하게될것이다 일것이다 비교적 좀 보다더 비하면 시키다 하게하다 할만하다 의해서 연이서 이어서 잇따라 포항 우도 양양 전주시 통영 제천 여수시 순천 고성 합천 한림읍 전주 경북 구좌읍 돌산읍 태안 하동 포항시 제주시 밀양 양평 울산 무무 뒤따라 뒤이어 결국 의지하여 기대여 통하여 자마자 더욱더 불구하고 얼마든지 마음대로 주저하지 않고 곧 즉시 바로 당장 하자마자 밖에 안된다 하면된다 그래 그렇지 요컨대 다시 말하자면 바꿔 춘천 층 남해 스테이 서울 여수 거제 추천 경주 곳 객실 이다 강원도 강원 홍천 부산 영도 호 객실 교동 풀빌라 빌라 풀 스튜디오 펜션 말하면 즉 구체적으로 말하자면 시작하여 시초에 이상 허 헉 허걱 바와같이 해도좋다 해도된다 게다가 더구나 하물며 와르르 팍 퍽 펄렁 동안 이래 하고있었다 이었다 에서 로부터 까지 예하면 했어요 해요 함께 같이 더불어 마저 마저도 양자 모두 습니다 가까스로 하려고하다 즈음하여 다른 다른 방면으로 해봐요 습니까 했어요 말할것도 없고 무릎쓰고 개의치않고 하는것만 못하다 하는것이 낫다 매 매번 들 모 어느것 어느 로써 갖고말하자면 어디 어느쪽 어느것 어느해 어느 년도 라 해도 언젠가 어떤것 어느것 저기 저쪽 저것 그때 그럼 그러면 요만한걸 그래 그때 저것만큼 그저 이르기까지 할 줄 안다 할 힘이 있다 너 너희 당신 어찌 설마 차라리 할지언정 할지라도 할망정 할지언정 구토하다 게우다 토하다 메쓰겁다 옆사람 퉤 쳇 의거하여 근거하여 의해 따라 힘입어 그 다음 버금 두번째로 기타 첫번째로 나머지는 그중에서 견지에서 형식으로 쓰여 입장에서 위해서 단지 의해되다 하도록시키다 뿐만아니라 반대로 전후 전자 앞의것 잠시 잠깐 하면서 그렇지만 다음에 그러한즉 그런즉 남들 아무거나 어찌하든지 같다 비슷하다 예컨대 이럴정도로 어떻게 만약 만일 위에서 서술한바와같이 인 듯하다 하지 않는다면 만약에 무엇 무슨 어느 어떤 아래윗 조차 한데 그럼에도 불구하고 여전히 심지어 까지도 조차도 하지 않도록 않기 위하여 때 시각 무렵 시간 동안 어때 어떠한 하여금 네 예 우선 누구 누가 알겠는가 아무도 줄은모른다 줄은 몰랏다 하는 김에 겸사겸사 하는바 그런 까닭에 한 이유는 그러니 그러니까 때문에 그 너희 그들 너희들 타인 것 것들 너 위하여 공동으로 동시에 하기 위하여 어찌하여 무엇때문에 붕붕 윙윙 나 우리 엉엉 휘익 윙윙 오호 아하 어쨋든 만 못하다 하기보다는 차라리 하는 편이 낫다 흐흐 놀라다 상대적으로 말하자면 마치 아니라면 쉿 그렇지 않으면 그렇지 않다면 안 그러면 아니었다면 하든지 아니면 이라면 좋아 알았어 하는것도 그만이다 어쩔수 없다 하나 일 일반적으로 일단 한켠으로는 오자마자 이렇게되면 이와같다면 전부 한마디 한항목 근거로 하기에 아울러 하지 않도록 않기 위해서 이르기까지 이 되다 로 인하여 까닭으로 이유만으로 이로 인하여 그래서 이 때문에 그러므로 그런 까닭에 알 수 있다 결론을 낼 수 있다 으로 인하여 있다 어떤것 관계가 있다 관련이 있다 연관되다 어떤것들 에 대해 이리하여 그리하여 여부 하기보다는 하느니 하면 할수록 운운 이러이러하다 하구나 하도다 다시말하면 다음으로 에 있다 에 달려 있다 우리 우리들 오히려 하기는한데 어떻게 어떻해 어찌됏어 어때 어째서 본대로 자 이 이쪽 여기 이것 이번 이렇게말하자면 이런 이러한 이와 같은 요만큼 요만한 것 얼마 안 되는 것 이만큼 이 정도의 이렇게 많은 것 이와 같다 이때 이렇구나 것과 같이 끼익 삐걱 따위 와 같은 사람들 부류의 사람들 왜냐하면 중의하나 오직 오로지 에 한하다 하기만 하면 도착하다 까지 미치다 도달하다 정도에 이르다 할 지경이다 결과에 이르다 관해서는 여러분 하고 있다 한 후 혼자 자기 자기집 자신 우에 종합한것과같이 총적으로 보면 총적으로 말하면 총적으로 대로 하다 으로서 참 그만이다 할 따름이다 쿵 탕탕 쾅쾅 둥둥 봐 봐라 아이야 아니 와아 응 아이 참나 년 월 일 령 영 일 이 삼 사 오 육 륙 칠 팔 구 이천육 이천칠 이천팔 이천구 하나 둘 셋 넷 다섯 여섯 일곱 여덟 아홉 령 영 이 있 하 것 들 그 되 수 이 보 않 없 나 사람 주 아니 등 같 우리 때 년 가 한 지 대하 오 말 일 그렇 위하 때문 그것 두 말하 알 그러나 받 못하 일 그런 또 문제 더 사회 많 그리고 좋 크 따르 중 나오 가지 씨 시키 만들 지금 생각하 그러 속 하나 집 살 모르 적 월 데 자신 안 어떤 내 내 경우 감성 숙소 호텔 제주 제주도 강원도 강릉 속초 속초시 에어비앤비 명 생각 시간 그녀 수 약 다시 이런 앞 보이 번 나 다른 어떻 여자 개 전 들 사실 이렇 점 싶 말 정도 좀 원 잘 통하 놓 사실 이렇 점 싶 말 정도 좀 원 잘 통하 ㅆ 재 채 독 롭 도란도 팅 감성비앤비 공간 참여 이벤트 당첨자 기간 부산 #부산숙소 #부산에어비앤비 #부산여행 경기 경주시 있는 한 #경주숙소 강원도 제주 제주시 #제주숙소 #제주감성숙소 #제주숙소추천 남해군 서면 춘천시 남해 광안리 편 반 안면도 산방산 님 사장 정말 날 꼭 찾다 직접 날 그냥 맘 근데 스타 욕 편 째 첫 맛 채 황리단 황리단길 미리 열다 거제도 공간 광안리 광안대교 스다 해운대 헤이 윤슬 울릉도 손 옆' #불용어 리스트 형성"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pk2WROfwrYSW"
      },
      "source": [
        "stop_words=stop_words.split(' ')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qpyB1Vb4x_D"
      },
      "source": [
        "stop_location = pd.read_csv('location_words.csv')\n",
        "stop_location.columns = ['index', 'location']\n",
        "stop_location = stop_location.location.tolist()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c92Pam2292uK"
      },
      "source": [
        "stop = stop_words + stop_location\n",
        "stop = pd.Series(stop)\n",
        "stopwords = stop.unique().tolist()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hs8x2Y7yMT_l"
      },
      "source": [
        "## 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TliirW6eSiM9"
      },
      "source": [
        "def tokenize_mec(sent): ## mec 토크나이저\n",
        "  result = []\n",
        "  word_s = mecab.pos(sent)\n",
        "  word = [word for word, tag in word_s if not tag.startswith('E') and not tag.startswith('J') and not tag.startswith('S') and word not in stopwords]\n",
        "  result = word\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlHUCumEPxJb"
      },
      "source": [
        "def tokenize_okt(txt) : # okt 토크나이저\n",
        "    result = []\n",
        "    word_s = okt.pos(txt, norm=True, stem=True)\n",
        "    for n, h in word_s :\n",
        "        if not (h in ['Noun', 'Verb', 'Adjective']) : continue\n",
        "        result.append(n)\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLz67C_-a3cf"
      },
      "source": [
        "data.reset_index(drop=True,inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wph8eaZFcdbZ"
      },
      "source": [
        "# data.content[66]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdvQI1TLMzbt"
      },
      "source": [
        "# tokenize_mec(data.content[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcL-OFxGapE5"
      },
      "source": [
        "data['token_content_mec'] = np.NaN\n",
        "data['token_content_okt'] = np.NaN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuetwR_TaaJ_",
        "outputId": "6b4cb938-c1bf-484e-adb2-63304a804dab"
      },
      "source": [
        "for i in range(len(data)):\n",
        "  data['token_content_mec'][i] = tokenize_mec(data.content[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:1732: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self._setitem_single_block(indexer, value, name)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODtQrXzuQGL4",
        "outputId": "ce5d47cc-75fa-4411-ac94-d9776f0493f7"
      },
      "source": [
        "for i in range(len(data)):\n",
        "  data['token_content_okt'][i] = tokenize_okt(data.content[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:1732: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self._setitem_single_block(indexer, value, name)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OE1CavJPnPDv"
      },
      "source": [
        "for i in range(len(data.content)): #불용어 제거\n",
        "  result = []\n",
        "  for w in data.token_content_okt[i]:\n",
        "    if w not in stop:\n",
        "      result.append(w)\n",
        "    data.token_content_okt[i] = result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "j26NR2X7oQl5"
      },
      "source": [
        "# data.token_content_mec[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQlCOafLl1qk"
      },
      "source": [
        "for i in range(len(data.content)): #불용어 제거\n",
        "  result = []\n",
        "  for w in data.token_content_mec[i]:\n",
        "    if w not in stop:\n",
        "      result.append(w)\n",
        "    data.token_content_mec[i] = result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22JUOS8Alt8K"
      },
      "source": [
        "# data.to_csv('./data.csv', encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AW9C899D02Lc"
      },
      "source": [
        "# LDA 모델링"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "AEjlRRIN-VtS",
        "outputId": "97c62d65-aa92-4ccd-d388-6886906b1c74"
      },
      "source": [
        "mdl = tp.LDAModel(k=20)\n",
        "\n",
        "for row in text :\n",
        "    mdl.add_doc(row.strip().split())\n",
        "\n",
        "for i in range(0, 100, 10):\n",
        "    mdl.train(10)\n",
        "    print('Iteration: {}\\tLog-likelihood: {}'.format(i, mdl.ll_per_word))\n",
        "\n",
        "for k in range(mdl.k):\n",
        "    print('Top 10 words of topic #{}'.format(k))\n",
        "    print(mdl.get_topic_words(k, top_n=10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-6d326ed62b36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Either `words` or `rawWords` must be filled."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "bbOmxGO1EUY_",
        "outputId": "6439aad4-173e-4efc-f8a7-cc2f5f0011bf"
      },
      "source": [
        "model = tp.LDAModel(k=20, alpha=0.1, eta=0.01, min_cf=5)\n",
        "\n",
        "for i, row in enumerate(text):\n",
        "    model.add_doc(tokenize(row)) # tokenize함수를 이용해 전처리한 결과를 add_doc에 넣습니다.\n",
        "    if i % 10 == 0: print('Document #{} has been loaded'.format(i))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-227a729d3605>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# tokenize함수를 이용해 전처리한 결과를 add_doc에 넣습니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Document #{} has been loaded'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Either `words` or `rawWords` must be filled."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "sWx4f6FQEa6x",
        "outputId": "81dc5876-b8de-47b6-b610-16db4b9cc847"
      },
      "source": [
        "model.train(0)\n",
        "print('Total docs:', len(model.docs))\n",
        "print('Total words:', model.num_words)\n",
        "print('Vocab size:', model.num_vocabs)\n",
        " \n",
        " \n",
        "for i in range(200):\n",
        "    print('Iteration {}\\tLL per word: {}'.format(i, model.ll_per_word))\n",
        "    model.train(1)\n",
        " \n",
        "for i in range(model.k):\n",
        "    res = model.get_topic_words(i, top_n=10)\n",
        "    print('Topic #{}'.format(i), end='\\t')\n",
        "    print(', '.join(w for w, p in res))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1ccc6969856c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Total docs:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Total words:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Vocab size:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_vocabs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI3svF-gF-23"
      },
      "source": [
        "# LDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c7JNaAmGBGX"
      },
      "source": [
        "## 딕셔너리 및 코퍼스 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCx1qcbpnUbU"
      },
      "source": [
        "# data = pd.read_csv('docs.csv', )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "L8I-6cnbqDTs",
        "outputId": "86e8ba87-b790-470b-8f8b-1a43f2cd96e8"
      },
      "source": [
        "# data.token_content_okt.split()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-f60eca72dd43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_content_okt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5485\u001b[0m         ):\n\u001b[1;32m   5486\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5487\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5489\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'split'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gg8-EV4SICFC"
      },
      "source": [
        "docs = data.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zg4RrA_DoMT0"
      },
      "source": [
        "# docs.to_csv('docs.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1YaB_wMnd2J"
      },
      "source": [
        "docs.token_content_okt[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lb8VFp3WpN8z"
      },
      "source": [
        "# [d.split() for d in docs.token_content_okt]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWVxqY20o4B9"
      },
      "source": [
        "docs.token_content_mec[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbAiJKdkyUHJ"
      },
      "source": [
        "np.re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "VzT_kOBdHarE",
        "outputId": "417adb84-2ea8-4129-d87a-0396c56ad7a8"
      },
      "source": [
        "from gensim import corpora\n",
        "dictionary_mec = corpora.Dictionary(docs.token_content_mec)\n",
        "corpus_mec = [dictionary.doc2bow(text) for text in docs.token_content_mec]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-4903466db432>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdictionary_mec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_content_mec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcorpus_mec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_content_mec\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/corpora/dictionary.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, documents, prune_at)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprune_at\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprune_at\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/corpora/dictionary.py\u001b[0m in \u001b[0;36madd_documents\u001b[0;34m(self, documents, prune_at)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0;31m# update Dictionary with the document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_update\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# ignore the result, here we only care about updating token ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         logger.info(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/corpora/dictionary.py\u001b[0m in \u001b[0;36mdoc2bow\u001b[0;34m(self, document, allow_update, return_missing)\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0mcounter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mtoken2id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken2id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: decoding to str: need a bytes-like object, list found"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASVoSag0InqG"
      },
      "source": [
        "from gensim import models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psqM5OToHsQA"
      },
      "source": [
        "## tf-idf 폼 적용\n",
        "tfidf_mec = models.TfidfModel(corpus_mec)\n",
        "corpus_mec = tfidf[corpus_mec]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjL1xWakIxX9"
      },
      "source": [
        "## 학습하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Mhp_GycIWoL"
      },
      "source": [
        "import gensim\n",
        "NUM_TOPICS = 20\n",
        "ldamodel_mec = gensim.models.ldamodel_mec.LdaModel(corpus_mec, num_topics = NUM_TOPICS, id2word=dictionary_mec, passes=15)\n",
        "topics_mec = ldamodel_mec.print_topics(num_words=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jji_4QPUJB-R"
      },
      "source": [
        "# pip install pyLDAvis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k77eY8YIKlqe"
      },
      "source": [
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "pyLDAvis.enable_notebook()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lm6fJlonMHxv"
      },
      "source": [
        "vis = gensimvis.prepare(ldamodel, corpus, dictionary)\n",
        "pyLDAvis.display(vis)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiCga2sXMScQ"
      },
      "source": [
        "#TOMOTOPY_LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVK2mPRU82qb",
        "outputId": "aa6482cd-d95e-4083-ff9c-c4c801a24c36"
      },
      "source": [
        "! pip install tomotopy #호출\n",
        "import tomotopy as tp"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tomotopy\n",
            "  Downloading tomotopy-0.12.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (16.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 16.3 MB 139 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tomotopy) (1.19.5)\n",
            "Installing collected packages: tomotopy\n",
            "Successfully installed tomotopy-0.12.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "17DUZs2QLxW-",
        "outputId": "789f3c14-9f7a-475c-8a55-611ce9feefff"
      },
      "source": [
        "for i in range(len(data.content)):\n",
        "  if len(data.token_content_okt[i]) <= 5:\n",
        "    print(i) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-56eb926074e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_content_okt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhVjUfRTPKQD"
      },
      "source": [
        "data = data.drop([data.index[1905], data.index[2101]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eq4BqvIG9Idy"
      },
      "source": [
        "model = tp.LDAModel(k=20, alpha=0.1, eta=0.01, min_cf=5)\n",
        "# LDAModel을 생성\n",
        "# 토픽의 개수(k)는 20개, alpha 파라미터는 0.1, eta 파라미터는 0.01\n",
        "# 전체 말뭉치에 5회 미만 등장한 단어들은 제거"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NC-roML9Wfb"
      },
      "source": [
        "for row in data.token_content_okt:\n",
        "    model.add_doc(row) # 행 별로 model에 추가합니다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmH5G7vo-9uf",
        "outputId": "23b72d80-f8fb-43b1-a3c7-d89fe35c80e9"
      },
      "source": [
        "# model의 num_words나 num_vocabs 등은 train을 시작해야 확정됩니다.\n",
        "# 따라서 이 값을 확인하기 위해서 train(0)을 하여 실제 train은 하지 않고\n",
        "# 학습 준비만 시킵니다.\n",
        "# num_words, num_vocabs에 관심 없다면 이부분은 생략해도 됩니다.\n",
        "model.train(0) \n",
        "print('Total docs:', len(model.docs))\n",
        "print('Total words:', model.num_words)\n",
        "print('Vocab size:', model.num_vocabs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total docs: 7693\n",
            "Total words: 3660832\n",
            "Vocab size: 15402\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeB6n38C_GS5"
      },
      "source": [
        "# 다음 구문은 train을 총 200회 반복하면서, \n",
        "# 매 단계별로 로그 가능도 값을 출력해줍니다.\n",
        "# 혹은 단순히 model.train(200)으로 200회 반복도 가능합니다.\n",
        "for i in range(200):\n",
        "    print('Iteration {}\\tLL per word: {}'.format(i, model.ll_per_word))\n",
        "    model.train(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6AxxsXw_Jxg",
        "outputId": "58ba8148-0dcc-4c7b-e158-a862454611e2"
      },
      "source": [
        "# 학습된 토픽들을 출력해보도록 합시다.\n",
        "for i in range(model.k):\n",
        "    # 토픽 개수가 총 20개이니, 0~19번까지의 토픽별 상위 단어 10개를 뽑아봅시다.\n",
        "    res = model.get_topic_words(i, top_n=10)\n",
        "    print('Topic #{}'.format(i), end='\\t')\n",
        "    print(', '.join(w for w, p in res))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Topic #0\t하다, 있다, 숙소, 좋다, 강릉, 먹다, 같다, 않다, 들다, 자다\n",
            "Topic #1\t하다, 있다, 곳, 것, 숙소, 같다, 좋다, 수, 공간, 감성\n",
            "Topic #2\t하다, 있다, 것, 없다, 같다, 않다, 보다, 방, 자다, 때\n",
            "Topic #3\t숙소, 에어비앤비, 하다, 있다, 감성, 예쁘다, 호스트, 집, 곳, 좋다\n",
            "Topic #4\t예약, 숙소, 스테이, 인, 하다, 기준, 가격, 감성, 최대, 박\n",
            "Topic #5\t하다, 이다, 있다, 수, 집, 공간, 되다, 보다, 마을, 만들다\n",
            "Topic #6\t제주, 숙소, 제주도, 하다, 감성, 있다, 여행, 좋다, 곳, 이다\n",
            "Topic #7\t하다, 진짜, 우리, 먹다, 넘다, 좋다, 찍다, 오다, 여기, 보다\n",
            "Topic #8\t먹다, 조식, 하다, 카페, 맛있다, 좋다, 있다, 가다, 예쁘다, 맛\n",
            "Topic #9\t있다, 하다, 되어다, 좋다, 공간, 수, 화장실, 주방, 준비, 층\n",
            "Topic #10\t하다, 있다, 펜션, 수영장, 아이, 좋다, 가족, 수, 놀다, 오다\n",
            "Topic #11\t하다, 우리, 보다, 여행, 좋다, 아침, 스테이, 날, 오다, 집\n",
            "Topic #12\t하다, 먹다, 준비, 커피, 있다, 집, 되어다, 날, 아침, 와인\n",
            "Topic #13\t뷰, 바다, 하다, 숙소, 보다, 오션, 층, 있다, 부산, 보이다\n",
            "Topic #14\t하다, 숙소, 좋다, 사진, 찍다, 저, 저희, 이쁘다, 진짜, 있다\n",
            "Topic #15\t하다, 바베큐, 펜션, 있다, 감성, 숙소, 고기, 먹다, 굽다, 캠핑\n",
            "Topic #16\t하다, 있다, 객실, 룸, 이용, 호텔, 되다, 체크, 수, 되어다\n",
            "Topic #17\t있다, 하다, 수, 옥, 숙소, 곳, 것, 좋다, 경주, 이다\n",
            "Topic #18\t하다, 남해, 있다, 풀, 빌라, 스파, 뷰, 펜션, 수, 여수\n",
            "Topic #19\t하다, 예약, 숙소, 탕, 사장, 노천, 있다, 날, 예쁘다, 비\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjLz56Iw_Pvf"
      },
      "source": [
        "# KIWI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSOP0pGeQifB"
      },
      "source": [
        "kiwidata = pd.read_csv('total_blog_dataset.csv')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOANissPRC21"
      },
      "source": [
        "kiwidata.drop('Unnamed: 0', axis=1, inplace=True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "dR7gMsm2R1a9",
        "outputId": "4f430ff4-fced-4236-90cf-2131fc413155"
      },
      "source": [
        "kiwidata"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>['여행', '다녀오다', '게으르다', '여행', '고민', '바라다', '올리다...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>['다녀오다', '후기', '휴가', '틈나다', '호캉스', '다녀오다', '다녀...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>['여행', '쓰다', '거', '고르다', '아무래도', '여행', '주제', '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>['오늘', '지나다', '휴가', '찐친', '다녀오다', '메이킷슬', '후기'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>['지인', '소개', '#시우재', '마침', '이것저것', '기념일', '재다'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3731</th>\n",
              "      <td>['국내', '여행지', '여행', '맨션자두', '들어가다', '커다랗다', '널...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3732</th>\n",
              "      <td>['영이', '분기', '여행', '스테이솔', '여행지', '편', '영이', '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3733</th>\n",
              "      <td>['다귀', '쉼터', '오늘', '째', '하얗다', '외관', '매듭', '스무...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3734</th>\n",
              "      <td>['아파트먼트', '여행', '자주', '불구', '잡다', '놀다', '거', '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3735</th>\n",
              "      <td>['포도봉', '미루다', '미루다', '숙소콕', '여행', '다행', '맞추다'...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3736 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                content\n",
              "0     ['여행', '다녀오다', '게으르다', '여행', '고민', '바라다', '올리다...\n",
              "1     ['다녀오다', '후기', '휴가', '틈나다', '호캉스', '다녀오다', '다녀...\n",
              "2     ['여행', '쓰다', '거', '고르다', '아무래도', '여행', '주제', '...\n",
              "3     ['오늘', '지나다', '휴가', '찐친', '다녀오다', '메이킷슬', '후기'...\n",
              "4     ['지인', '소개', '#시우재', '마침', '이것저것', '기념일', '재다'...\n",
              "...                                                 ...\n",
              "3731  ['국내', '여행지', '여행', '맨션자두', '들어가다', '커다랗다', '널...\n",
              "3732  ['영이', '분기', '여행', '스테이솔', '여행지', '편', '영이', '...\n",
              "3733  ['다귀', '쉼터', '오늘', '째', '하얗다', '외관', '매듭', '스무...\n",
              "3734  ['아파트먼트', '여행', '자주', '불구', '잡다', '놀다', '거', '...\n",
              "3735  ['포도봉', '미루다', '미루다', '숙소콕', '여행', '다행', '맞추다'...\n",
              "\n",
              "[3736 rows x 1 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7xjMEiBSsdR"
      },
      "source": [
        "#\"\"\"한글빼고 전부 제거\"\"\"\n",
        "def sub_special(s):\n",
        "  return re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣]',' ',s).strip().split()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71f3huevTo44"
      },
      "source": [
        "for i in range(len(kiwidata)):\n",
        "  kiwidata.content[i] = sub_special(kiwidata.content[i])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIG8pgA5UKzw"
      },
      "source": [
        "for i in range(len(kiwidata)): #불용어 제거\n",
        "  result = []\n",
        "  for w in kiwidata.content[i]:\n",
        "    if w not in stopwords:\n",
        "      result.append(w)\n",
        "    kiwidata.content[i] = result"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJvUCsT1UcNb"
      },
      "source": [
        "sample = kiwidata.copy()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEpCtJe7WgCt"
      },
      "source": [
        ""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3AkmoStWmyV"
      },
      "source": [
        "#TOMOTOPY_LDA_KIWI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jLEL0wYWmyW",
        "outputId": "210d6281-1517-4a0f-8162-2c43da50c34c"
      },
      "source": [
        "! pip install tomotopy #호출\n",
        "import tomotopy as tp"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tomotopy in /usr/local/lib/python3.7/dist-packages (0.12.2)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tomotopy) (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1EIcf3rWmyW",
        "outputId": "25bc2c22-e192-4774-9692-143ab5ab48e7"
      },
      "source": [
        "for i in range(len(sample.content)): # 빈 리스트 확인\n",
        "  if len(sample.content[i]) <= 5:\n",
        "    print(i)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1855\n",
            "3400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTNJzuSWWmyW"
      },
      "source": [
        "sample = sample.drop([sample.index[1855],sample.index[3400]])"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCmFFB8TbIBu"
      },
      "source": [
        "sample.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "6HqPfZoseNDI",
        "outputId": "27983655-1320-444c-9319-99e25c0118fd"
      },
      "source": [
        "sample"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[여행, 다녀오다, 게으르다, 여행, 고민, 바라다, 올리다, 오늘, 리뷰, 편안,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[다녀오다, 후기, 휴가, 틈나다, 호캉스, 다녀오다, 다녀오다, 주변, 아고다, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[여행, 고르다, 아무래도, 여행, 주제, 자체, 쉬, 편안, 쉬다, 중요, 부분,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[오늘, 지나다, 휴가, 찐친, 다녀오다, 메이킷슬, 후기, 돈, 산, 인테리어, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[지인, 소개, 시우재, 마침, 이것저것, 기념일, 재다, 다녀오다, 멀리, 놀다,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3729</th>\n",
              "      <td>[국내, 여행지, 여행, 맨션자두, 들어가다, 커다랗다, 널판지, 깔다, 발, 편하...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3730</th>\n",
              "      <td>[영이, 분기, 여행, 스테이솔, 여행지, 영이, 퇴근, 오빠, 먼저, 카페, 데이...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3731</th>\n",
              "      <td>[다귀, 쉼터, 오늘, 하얗다, 외관, 매듭, 스무, 시장, 포장, 시장, 네비찍,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3732</th>\n",
              "      <td>[아파트먼트, 여행, 자주, 불구, 잡다, 놀다, 처음, 고르다, 급하다, 잡다, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3733</th>\n",
              "      <td>[포도봉, 미루다, 미루다, 숙소콕, 여행, 다행, 맞추다, 다녀오다, 예쁘다, 룰...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3734 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                content\n",
              "0     [여행, 다녀오다, 게으르다, 여행, 고민, 바라다, 올리다, 오늘, 리뷰, 편안,...\n",
              "1     [다녀오다, 후기, 휴가, 틈나다, 호캉스, 다녀오다, 다녀오다, 주변, 아고다, ...\n",
              "2     [여행, 고르다, 아무래도, 여행, 주제, 자체, 쉬, 편안, 쉬다, 중요, 부분,...\n",
              "3     [오늘, 지나다, 휴가, 찐친, 다녀오다, 메이킷슬, 후기, 돈, 산, 인테리어, ...\n",
              "4     [지인, 소개, 시우재, 마침, 이것저것, 기념일, 재다, 다녀오다, 멀리, 놀다,...\n",
              "...                                                 ...\n",
              "3729  [국내, 여행지, 여행, 맨션자두, 들어가다, 커다랗다, 널판지, 깔다, 발, 편하...\n",
              "3730  [영이, 분기, 여행, 스테이솔, 여행지, 영이, 퇴근, 오빠, 먼저, 카페, 데이...\n",
              "3731  [다귀, 쉼터, 오늘, 하얗다, 외관, 매듭, 스무, 시장, 포장, 시장, 네비찍,...\n",
              "3732  [아파트먼트, 여행, 자주, 불구, 잡다, 놀다, 처음, 고르다, 급하다, 잡다, ...\n",
              "3733  [포도봉, 미루다, 미루다, 숙소콕, 여행, 다행, 맞추다, 다녀오다, 예쁘다, 룰...\n",
              "\n",
              "[3734 rows x 1 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zK7R2l8WmyW"
      },
      "source": [
        "model = tp.LDAModel(k=10, alpha=0.1, eta=0.01, min_cf=3, )\n",
        "# LDAModel을 생성\n",
        "# 토픽의 개수(k)는 20개, alpha 파라미터는 0.1, eta 파라미터는 0.01\n",
        "# 전체 말뭉치에 5회 미만 등장한 단어들은 제거"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vcfup1psWmyW"
      },
      "source": [
        "for row in sample.content:\n",
        "    model.add_doc(row) # 행 별로 model에 추가합니다."
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPiLODgGWmyW",
        "outputId": "f4ac25bf-f1c7-4cf5-a1b4-a80ffe4f040f"
      },
      "source": [
        "# model의 num_words나 num_vocabs 등은 train을 시작해야 확정됩니다.\n",
        "# 따라서 이 값을 확인하기 위해서 train(0)을 하여 실제 train은 하지 않고\n",
        "# 학습 준비만 시킵니다.\n",
        "# num_words, num_vocabs에 관심 없다면 이부분은 생략해도 됩니다.\n",
        "model.train(0) \n",
        "print('Total docs:', len(model.docs))\n",
        "print('Total words:', model.num_words)\n",
        "print('Vocab size:', model.num_vocabs)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total docs: 3734\n",
            "Total words: 818819\n",
            "Vocab size: 14214\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOInH6DZWmyW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1be4a74e-07af-4f0a-c398-84caef6f73d2"
      },
      "source": [
        "# 다음 구문은 train을 총 200회 반복하면서, \n",
        "# 매 단계별로 로그 가능도 값을 출력해줍니다.\n",
        "# 혹은 단순히 model.train(200)으로 200회 반복도 가능합니다.\n",
        "for i in range(200):\n",
        "    print('Iteration {}\\tLL per word: {}'.format(i, model.ll_per_word))\n",
        "    model.train(1)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0\tLL per word: -10.543898160006622\n",
            "Iteration 1\tLL per word: -10.211385061618149\n",
            "Iteration 2\tLL per word: -9.998708009420376\n",
            "Iteration 3\tLL per word: -9.842936962485489\n",
            "Iteration 4\tLL per word: -9.720396485174836\n",
            "Iteration 5\tLL per word: -9.620372695862311\n",
            "Iteration 6\tLL per word: -9.536629529676244\n",
            "Iteration 7\tLL per word: -9.4694801079685\n",
            "Iteration 8\tLL per word: -9.41091244248401\n",
            "Iteration 9\tLL per word: -9.355471453893523\n",
            "Iteration 10\tLL per word: -9.284557975962379\n",
            "Iteration 11\tLL per word: -9.259900930639036\n",
            "Iteration 12\tLL per word: -9.23059663892995\n",
            "Iteration 13\tLL per word: -9.198312253918331\n",
            "Iteration 14\tLL per word: -9.163049119018824\n",
            "Iteration 15\tLL per word: -9.127561660184\n",
            "Iteration 16\tLL per word: -9.092437558025727\n",
            "Iteration 17\tLL per word: -9.062928267434328\n",
            "Iteration 18\tLL per word: -9.032036143158937\n",
            "Iteration 19\tLL per word: -9.008149020433285\n",
            "Iteration 20\tLL per word: -8.979101825706445\n",
            "Iteration 21\tLL per word: -8.963966996196584\n",
            "Iteration 22\tLL per word: -8.945732457549802\n",
            "Iteration 23\tLL per word: -8.923199642710332\n",
            "Iteration 24\tLL per word: -8.902243036751454\n",
            "Iteration 25\tLL per word: -8.88569475299879\n",
            "Iteration 26\tLL per word: -8.867189337469924\n",
            "Iteration 27\tLL per word: -8.851699380000012\n",
            "Iteration 28\tLL per word: -8.836369869591545\n",
            "Iteration 29\tLL per word: -8.818194622318675\n",
            "Iteration 30\tLL per word: -8.804359573296425\n",
            "Iteration 31\tLL per word: -8.793414020060926\n",
            "Iteration 32\tLL per word: -8.783201908883552\n",
            "Iteration 33\tLL per word: -8.772442148526144\n",
            "Iteration 34\tLL per word: -8.760356396952233\n",
            "Iteration 35\tLL per word: -8.748307616706349\n",
            "Iteration 36\tLL per word: -8.738350491613767\n",
            "Iteration 37\tLL per word: -8.728839559169117\n",
            "Iteration 38\tLL per word: -8.719851941250049\n",
            "Iteration 39\tLL per word: -8.708753083574246\n",
            "Iteration 40\tLL per word: -8.69905526306666\n",
            "Iteration 41\tLL per word: -8.694019018553146\n",
            "Iteration 42\tLL per word: -8.685109323852538\n",
            "Iteration 43\tLL per word: -8.679126945661475\n",
            "Iteration 44\tLL per word: -8.669417875868186\n",
            "Iteration 45\tLL per word: -8.662703200181962\n",
            "Iteration 46\tLL per word: -8.656645581026545\n",
            "Iteration 47\tLL per word: -8.649948315723398\n",
            "Iteration 48\tLL per word: -8.642643881801579\n",
            "Iteration 49\tLL per word: -8.637200523180093\n",
            "Iteration 50\tLL per word: -8.631542675925978\n",
            "Iteration 51\tLL per word: -8.627933957349248\n",
            "Iteration 52\tLL per word: -8.621515464483707\n",
            "Iteration 53\tLL per word: -8.617102432652972\n",
            "Iteration 54\tLL per word: -8.612873702471221\n",
            "Iteration 55\tLL per word: -8.608680268908783\n",
            "Iteration 56\tLL per word: -8.604019906471205\n",
            "Iteration 57\tLL per word: -8.600904550100008\n",
            "Iteration 58\tLL per word: -8.596668202218272\n",
            "Iteration 59\tLL per word: -8.594032218577922\n",
            "Iteration 60\tLL per word: -8.58916042074867\n",
            "Iteration 61\tLL per word: -8.584854864752458\n",
            "Iteration 62\tLL per word: -8.57964674070381\n",
            "Iteration 63\tLL per word: -8.577545126061002\n",
            "Iteration 64\tLL per word: -8.574204551785693\n",
            "Iteration 65\tLL per word: -8.570792341965577\n",
            "Iteration 66\tLL per word: -8.567676584149735\n",
            "Iteration 67\tLL per word: -8.56522950009802\n",
            "Iteration 68\tLL per word: -8.566236696611005\n",
            "Iteration 69\tLL per word: -8.561073749172925\n",
            "Iteration 70\tLL per word: -8.559087371776608\n",
            "Iteration 71\tLL per word: -8.556587712576327\n",
            "Iteration 72\tLL per word: -8.552268180877459\n",
            "Iteration 73\tLL per word: -8.551406465953294\n",
            "Iteration 74\tLL per word: -8.547534459434036\n",
            "Iteration 75\tLL per word: -8.546019356513316\n",
            "Iteration 76\tLL per word: -8.543666038356777\n",
            "Iteration 77\tLL per word: -8.539874877496613\n",
            "Iteration 78\tLL per word: -8.535340027943624\n",
            "Iteration 79\tLL per word: -8.53134432765516\n",
            "Iteration 80\tLL per word: -8.529381654312072\n",
            "Iteration 81\tLL per word: -8.526492359934597\n",
            "Iteration 82\tLL per word: -8.523135279233752\n",
            "Iteration 83\tLL per word: -8.521434495536061\n",
            "Iteration 84\tLL per word: -8.519605981477623\n",
            "Iteration 85\tLL per word: -8.51668601695567\n",
            "Iteration 86\tLL per word: -8.515268475819328\n",
            "Iteration 87\tLL per word: -8.514291788575058\n",
            "Iteration 88\tLL per word: -8.511009556373487\n",
            "Iteration 89\tLL per word: -8.508800698051916\n",
            "Iteration 90\tLL per word: -8.505465416659847\n",
            "Iteration 91\tLL per word: -8.503117185979438\n",
            "Iteration 92\tLL per word: -8.50066220277128\n",
            "Iteration 93\tLL per word: -8.499081965211998\n",
            "Iteration 94\tLL per word: -8.49800470627939\n",
            "Iteration 95\tLL per word: -8.49696120428429\n",
            "Iteration 96\tLL per word: -8.496559922700655\n",
            "Iteration 97\tLL per word: -8.492811161857462\n",
            "Iteration 98\tLL per word: -8.493795312478554\n",
            "Iteration 99\tLL per word: -8.489145918371715\n",
            "Iteration 100\tLL per word: -8.48789466992642\n",
            "Iteration 101\tLL per word: -8.485725502187476\n",
            "Iteration 102\tLL per word: -8.482741956742379\n",
            "Iteration 103\tLL per word: -8.481477462009716\n",
            "Iteration 104\tLL per word: -8.48052927563324\n",
            "Iteration 105\tLL per word: -8.478288342411366\n",
            "Iteration 106\tLL per word: -8.476811100529966\n",
            "Iteration 107\tLL per word: -8.47574508903506\n",
            "Iteration 108\tLL per word: -8.475901818464376\n",
            "Iteration 109\tLL per word: -8.473458051810153\n",
            "Iteration 110\tLL per word: -8.473157434117402\n",
            "Iteration 111\tLL per word: -8.471491429135968\n",
            "Iteration 112\tLL per word: -8.468262015524695\n",
            "Iteration 113\tLL per word: -8.466867535030655\n",
            "Iteration 114\tLL per word: -8.465265586986822\n",
            "Iteration 115\tLL per word: -8.464284803629193\n",
            "Iteration 116\tLL per word: -8.462980728010645\n",
            "Iteration 117\tLL per word: -8.462038594895533\n",
            "Iteration 118\tLL per word: -8.461590026473335\n",
            "Iteration 119\tLL per word: -8.459628899648553\n",
            "Iteration 120\tLL per word: -8.45747276271572\n",
            "Iteration 121\tLL per word: -8.456594658358657\n",
            "Iteration 122\tLL per word: -8.456785523492242\n",
            "Iteration 123\tLL per word: -8.45450330349014\n",
            "Iteration 124\tLL per word: -8.454060599149997\n",
            "Iteration 125\tLL per word: -8.453951796433556\n",
            "Iteration 126\tLL per word: -8.452994516190811\n",
            "Iteration 127\tLL per word: -8.451692308562315\n",
            "Iteration 128\tLL per word: -8.450081036182297\n",
            "Iteration 129\tLL per word: -8.44786732005701\n",
            "Iteration 130\tLL per word: -8.44858210985696\n",
            "Iteration 131\tLL per word: -8.44686890287208\n",
            "Iteration 132\tLL per word: -8.445902881253156\n",
            "Iteration 133\tLL per word: -8.44457746493434\n",
            "Iteration 134\tLL per word: -8.444519598665767\n",
            "Iteration 135\tLL per word: -8.442914702508252\n",
            "Iteration 136\tLL per word: -8.441486673231166\n",
            "Iteration 137\tLL per word: -8.440227456830574\n",
            "Iteration 138\tLL per word: -8.440027456374072\n",
            "Iteration 139\tLL per word: -8.43875074173587\n",
            "Iteration 140\tLL per word: -8.439325690727335\n",
            "Iteration 141\tLL per word: -8.437996583485143\n",
            "Iteration 142\tLL per word: -8.436748675824488\n",
            "Iteration 143\tLL per word: -8.434226909712486\n",
            "Iteration 144\tLL per word: -8.433577876985888\n",
            "Iteration 145\tLL per word: -8.434323950426762\n",
            "Iteration 146\tLL per word: -8.435164577510184\n",
            "Iteration 147\tLL per word: -8.433940850188211\n",
            "Iteration 148\tLL per word: -8.43268543863492\n",
            "Iteration 149\tLL per word: -8.432527981211134\n",
            "Iteration 150\tLL per word: -8.432132295680837\n",
            "Iteration 151\tLL per word: -8.43130042053365\n",
            "Iteration 152\tLL per word: -8.429078258143209\n",
            "Iteration 153\tLL per word: -8.4285673240405\n",
            "Iteration 154\tLL per word: -8.429929756793523\n",
            "Iteration 155\tLL per word: -8.426743179166687\n",
            "Iteration 156\tLL per word: -8.426635388914375\n",
            "Iteration 157\tLL per word: -8.427104476401187\n",
            "Iteration 158\tLL per word: -8.425103677420893\n",
            "Iteration 159\tLL per word: -8.423472509929502\n",
            "Iteration 160\tLL per word: -8.42578467504673\n",
            "Iteration 161\tLL per word: -8.424797225930288\n",
            "Iteration 162\tLL per word: -8.423137770404615\n",
            "Iteration 163\tLL per word: -8.420370823755984\n",
            "Iteration 164\tLL per word: -8.41878698945105\n",
            "Iteration 165\tLL per word: -8.419222962758328\n",
            "Iteration 166\tLL per word: -8.419907983412253\n",
            "Iteration 167\tLL per word: -8.417646882723199\n",
            "Iteration 168\tLL per word: -8.416477359160114\n",
            "Iteration 169\tLL per word: -8.416915567863045\n",
            "Iteration 170\tLL per word: -8.416779046272767\n",
            "Iteration 171\tLL per word: -8.41443549734375\n",
            "Iteration 172\tLL per word: -8.41289964179275\n",
            "Iteration 173\tLL per word: -8.412059001004744\n",
            "Iteration 174\tLL per word: -8.412671800554971\n",
            "Iteration 175\tLL per word: -8.410058484204097\n",
            "Iteration 176\tLL per word: -8.410171022418062\n",
            "Iteration 177\tLL per word: -8.411254576089274\n",
            "Iteration 178\tLL per word: -8.409541577592746\n",
            "Iteration 179\tLL per word: -8.408375503964018\n",
            "Iteration 180\tLL per word: -8.410203301734322\n",
            "Iteration 181\tLL per word: -8.410506876623073\n",
            "Iteration 182\tLL per word: -8.4081252861086\n",
            "Iteration 183\tLL per word: -8.405089108125223\n",
            "Iteration 184\tLL per word: -8.404943620589554\n",
            "Iteration 185\tLL per word: -8.403322396879492\n",
            "Iteration 186\tLL per word: -8.403313216413606\n",
            "Iteration 187\tLL per word: -8.402613708995768\n",
            "Iteration 188\tLL per word: -8.404086212795386\n",
            "Iteration 189\tLL per word: -8.403695574838613\n",
            "Iteration 190\tLL per word: -8.401401101922168\n",
            "Iteration 191\tLL per word: -8.40205031466631\n",
            "Iteration 192\tLL per word: -8.402432145247156\n",
            "Iteration 193\tLL per word: -8.401837237070835\n",
            "Iteration 194\tLL per word: -8.39946706667901\n",
            "Iteration 195\tLL per word: -8.400666034585852\n",
            "Iteration 196\tLL per word: -8.398858221545666\n",
            "Iteration 197\tLL per word: -8.398670455817447\n",
            "Iteration 198\tLL per word: -8.397922249960951\n",
            "Iteration 199\tLL per word: -8.398929975858923\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2wD0hd_WmyX",
        "outputId": "5cb24d58-cb0e-4fa0-ce92-d5b90c8cb732"
      },
      "source": [
        "# 학습된 토픽들을 출력해보도록 합시다.\n",
        "for i in range(model.k):\n",
        "    # 토픽 개수가 총 20개이니, 0~19번까지의 토픽별 상위 단어 10개를 뽑아봅시다.\n",
        "    res = model.get_topic_words(i, top_n=10)\n",
        "    print('Topic #{}'.format(i), end='\\t')\n",
        "    print(', '.join(w for w, p in res))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic #0\t먹다, 찍다, 예쁘다, 넘다, 이쁘다, 아침, 여행, 맛있다, 못, 날씨\n",
            "Topic #1\t여행, 넓다, 거리, 거실, 욕실, 마당, 스럽, 침대, 느끼다, 돌담\n",
            "Topic #2\t여행, 예쁘다, 가족, 마음, 좋아하다, 엄마, 없이, 요즘, 놀다, 꽃\n",
            "Topic #3\t한옥, 여행, 숙박, 소개, 즐기다, 느끼다, 마당, 유명, 아래, 국내\n",
            "Topic #4\t바다, 오션뷰, 카페, 해변, 테라스, 여행, 해수욕장, 건물, 노을, 올라가다\n",
            "Topic #5\t먹다, 찍다, 친구, 들어가다, 못, 아쉽다, 주차, 깨끗, 근처, 후기\n",
            "Topic #6\t조식, 먹다, 카페, 제공, 맛있다, 아침, 커피, 음료, 체크인, 여행\n",
            "Topic #7\t바베큐, 수영장, 굽다, 탕, 야외, 놀다, 먹다, 자쿠지, 즐기다, 캠핑\n",
            "Topic #8\t예쁘다, 침대, 문, 커피, 침실, 사용, 거실, 테이블, 들어가다, 조명\n",
            "Topic #9\t풍경, 느끼다, 즐기다, 분위기, 힐링, 마음, 보내다, 가득, 편안, 여유\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBAueqUoSSl7"
      },
      "source": [
        "## 작업중"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAtGyU9zQy4A"
      },
      "source": [
        "extractor = tp.label.PMIExtractor(min_cf=10, min_df=5, max_len=5, max_cand=10000)\n",
        "cands = extractor.extract(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwbeZ4_eQ0V1"
      },
      "source": [
        "labeler = tp.label.FoRelevance(model, cands, min_df=5, smoothing=1e-2, mu=0.25)\n",
        "for k in range(model.k):\n",
        "    print(\"== Topic #{} ==\".format(k))\n",
        "    print(\"Labels:\", ', '.join(label for label, score in labeler.get_topic_labels(k, top_n=5)))\n",
        "    for word, prob in model.get_topic_words(k, top_n=10):\n",
        "        print(word, prob, sep='\\t')\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfrebpiWbWsy"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94abENWgkPRu"
      },
      "source": [
        "model.save('Kiwi_model', full=True) #모델 저장  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_ClxwBUk3cJ",
        "outputId": "2cbe4078-b8d8-439a-b4c1-74b1e565fb00"
      },
      "source": [
        "model.load('Kiwi_model') # 모델 확인"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tomotopy.LDAModel at 0x7f1557d72a30>"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TZ9SixPcHwX",
        "outputId": "c524e21a-e726-477a-d4f5-32b7c5a84f4a"
      },
      "source": [
        "topic_term_dists = np.stack([model.get_topic_word_dist(k) for k in range(model.k)])\n",
        "doc_topic_dists = np.stack([doc.get_topic_dist() for doc in model.docs])\n",
        "doc_topic_dists /= doc_topic_dists.sum(axis=1, keepdims=True)\n",
        "doc_lengths = np.array([len(doc.words) for doc in model.docs])\n",
        "vocab = list(model.used_vocabs)\n",
        "term_frequency = model.used_vocab_freq\n",
        "\n",
        "prepared_data = pyLDAvis.prepare(\n",
        "    topic_term_dists, \n",
        "    doc_topic_dists, \n",
        "    doc_lengths, \n",
        "    vocab, \n",
        "    term_frequency,\n",
        "    start_index=0, # tomotopy starts topic ids with 0, pyLDAvis with 1\n",
        "    sort_topics=False # IMPORTANT: otherwise the topic_ids between pyLDAvis and tomotopy are not matching!\n",
        ")\n",
        "pyLDAvis.save_html(prepared_data, 'ldavis.html')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pyLDAvis/_prepare.py:247: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  by='saliency', ascending=False).head(R).drop('saliency', 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXdnwWnXbzeZ"
      },
      "source": [
        "# 문헌별 토픽 분류"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyUYlXjvINqr"
      },
      "source": [
        "# 먼저 토픽 개수만큼 공간을 만들어줍니다.\n",
        "buckets = [[] for _ in range(model.k)]\n",
        "# 전체 문헌에 대해, top 1 주제 번호를 찾아 그 그룹에 해당 문헌을 포함시킵니다.\n",
        "for i, d in enumerate(model.docs):\n",
        "    buckets[d.get_topics(top_n=1)[0][0]].append([i,d])"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIEkHWHNISch"
      },
      "source": [
        "topic_table = pd.DataFrame() #테이블 생성\n",
        "doc_index = []\n",
        "content = []\n",
        "topics = []\n",
        "for i in range(len(model.docs)): # 인덱스별 값을 개별 변수로 테이블에 배정\n",
        "  doc_index = int(i)\n",
        "  content = sample.content[i]\n",
        "  topics = int(model.docs[i].get_topics(top_n=1)[0][0])\n",
        "  topic_table = topic_table.append(pd.Series([doc_index, content, topics]), ignore_index=True)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3Zw1u244X-w"
      },
      "source": [
        "topic_table.columns = ['index', 'content', 'topic']"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "PO4SZZn65hth",
        "outputId": "841984f7-b5f1-4bf9-eba6-911cdacb644f"
      },
      "source": [
        "topic_table"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>content</th>\n",
              "      <th>topic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>[여행, 다녀오다, 게으르다, 여행, 고민, 바라다, 올리다, 오늘, 리뷰, 편안,...</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>[다녀오다, 후기, 휴가, 틈나다, 호캉스, 다녀오다, 다녀오다, 주변, 아고다, ...</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.0</td>\n",
              "      <td>[여행, 고르다, 아무래도, 여행, 주제, 자체, 쉬, 편안, 쉬다, 중요, 부분,...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.0</td>\n",
              "      <td>[오늘, 지나다, 휴가, 찐친, 다녀오다, 메이킷슬, 후기, 돈, 산, 인테리어, ...</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4.0</td>\n",
              "      <td>[지인, 소개, 시우재, 마침, 이것저것, 기념일, 재다, 다녀오다, 멀리, 놀다,...</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3729</th>\n",
              "      <td>3729.0</td>\n",
              "      <td>[국내, 여행지, 여행, 맨션자두, 들어가다, 커다랗다, 널판지, 깔다, 발, 편하...</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3730</th>\n",
              "      <td>3730.0</td>\n",
              "      <td>[영이, 분기, 여행, 스테이솔, 여행지, 영이, 퇴근, 오빠, 먼저, 카페, 데이...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3731</th>\n",
              "      <td>3731.0</td>\n",
              "      <td>[다귀, 쉼터, 오늘, 하얗다, 외관, 매듭, 스무, 시장, 포장, 시장, 네비찍,...</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3732</th>\n",
              "      <td>3732.0</td>\n",
              "      <td>[아파트먼트, 여행, 자주, 불구, 잡다, 놀다, 처음, 고르다, 급하다, 잡다, ...</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3733</th>\n",
              "      <td>3733.0</td>\n",
              "      <td>[포도봉, 미루다, 미루다, 숙소콕, 여행, 다행, 맞추다, 다녀오다, 예쁘다, 룰...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3734 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       index                                            content  topic\n",
              "0        0.0  [여행, 다녀오다, 게으르다, 여행, 고민, 바라다, 올리다, 오늘, 리뷰, 편안,...    8.0\n",
              "1        1.0  [다녀오다, 후기, 휴가, 틈나다, 호캉스, 다녀오다, 다녀오다, 주변, 아고다, ...    8.0\n",
              "2        2.0  [여행, 고르다, 아무래도, 여행, 주제, 자체, 쉬, 편안, 쉬다, 중요, 부분,...    0.0\n",
              "3        3.0  [오늘, 지나다, 휴가, 찐친, 다녀오다, 메이킷슬, 후기, 돈, 산, 인테리어, ...    8.0\n",
              "4        4.0  [지인, 소개, 시우재, 마침, 이것저것, 기념일, 재다, 다녀오다, 멀리, 놀다,...    5.0\n",
              "...      ...                                                ...    ...\n",
              "3729  3729.0  [국내, 여행지, 여행, 맨션자두, 들어가다, 커다랗다, 널판지, 깔다, 발, 편하...    8.0\n",
              "3730  3730.0  [영이, 분기, 여행, 스테이솔, 여행지, 영이, 퇴근, 오빠, 먼저, 카페, 데이...    0.0\n",
              "3731  3731.0  [다귀, 쉼터, 오늘, 하얗다, 외관, 매듭, 스무, 시장, 포장, 시장, 네비찍,...    8.0\n",
              "3732  3732.0  [아파트먼트, 여행, 자주, 불구, 잡다, 놀다, 처음, 고르다, 급하다, 잡다, ...    8.0\n",
              "3733  3733.0  [포도봉, 미루다, 미루다, 숙소콕, 여행, 다행, 맞추다, 다녀오다, 예쁘다, 룰...    0.0\n",
              "\n",
              "[3734 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKJzz5Lmem8b"
      },
      "source": [
        "#KoBERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9icwN1VkiX9"
      },
      "source": [
        "!pip install mxnet\n",
        "!pip install gluonnlp pandas tqdm\n",
        "!pip install sentencepiece\n",
        "!pip install transformers==3.0.2\n",
        "!pip install torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bt1-_BRikixw"
      },
      "source": [
        "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHSdD59lkldm"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gluonnlp as nlp\n",
        "import numpy as np\n",
        "from tqdm import tqdm, tqdm_notebook"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESifVzj5knVL"
      },
      "source": [
        "#kobert\n",
        "from kobert.utils import get_tokenizer\n",
        "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
        "\n",
        "#transformers\n",
        "from transformers import AdamW\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVazoLjIkpN1"
      },
      "source": [
        "#GPU 사용\n",
        "device = torch.device(\"cuda:0\")"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKNf7ryWktT6",
        "outputId": "b9293532-111b-4d0d-8727-6db4a2bdb547"
      },
      "source": [
        "#BERT 모델, Vocabulary 불러오기\n",
        "bertmodel, vocab = get_pytorch_kobert_model()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[██████████████████████████████████████████████████]\n",
            "[██████████████████████████████████████████████████]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLPncZ7DqLKN"
      },
      "source": [
        "data_list = topic_table.copy()"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBtY_9Zgqpgm"
      },
      "source": [
        "data_list.drop('index', axis=1, inplace=True)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0eTA1bJq95Q"
      },
      "source": [
        "data_list.topic = data_list.topic.astype('int')"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EteAqQorRWp",
        "outputId": "322445b2-0422-4a2e-e394-349f41705c41"
      },
      "source": [
        "data_list.info()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3734 entries, 0 to 3733\n",
            "Data columns (total 2 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   content  3734 non-null   object\n",
            " 1   topic    3734 non-null   int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 58.5+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmNQuirMschZ"
      },
      "source": [
        "data_list['str_content'] = np.NaN"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tD6rG0C2sFvc",
        "outputId": "61798262-163d-4538-9f67-8b2790e9b56a"
      },
      "source": [
        "for i in range(len(data_list)):\n",
        "  data_list['str_content'][i] = \" \".join(data_list.content[i])"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  iloc._setitem_with_indexer(indexer, value)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZu9BkJYrrHB"
      },
      "source": [
        "topic_list = []\n",
        "for q, label in zip(data_list['str_content'], data_list['topic'])  :\n",
        "    data = []\n",
        "    data.append(q)\n",
        "    data.append(str(label))\n",
        "\n",
        "    topic_list.append(data)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-jjTRbIso7O"
      },
      "source": [
        "topic_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mG1IKFQUlX4k"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "                                                         \n",
        "dataset_train, dataset_test = train_test_split(topic_list, test_size=0.4, random_state=0)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRhYKMQrpG7G"
      },
      "source": [
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
        "                 pad, pair):\n",
        "        transform = nlp.data.BERTSentenceTransform(\n",
        "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
        "\n",
        "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
        "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return (self.sentences[i] + (self.labels[i], ))\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.labels))"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6zYzN_aqbqU"
      },
      "source": [
        "# Setting parameters\n",
        "max_len = 64\n",
        "batch_size = 32\n",
        "warmup_ratio = 0.1\n",
        "num_epochs = 10\n",
        "max_grad_norm = 1\n",
        "log_interval = 200\n",
        "learning_rate =  0.05"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5bYHDIVqdQm",
        "outputId": "5887e6c2-be77-4082-f7b3-7d7b7488b49f"
      },
      "source": [
        "#토큰화\n",
        "tokenizer = get_tokenizer()\n",
        "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
        "\n",
        "data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n",
        "data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using cached model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaXY5HN1qelT",
        "outputId": "fc49a4e0-c43b-4e15-9a0e-efe03c6d148b"
      },
      "source": [
        "data_train[0] # 확인"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([   2,  795, 6642, 6607, 1165, 7147, 6896, 6855, 6441, 6909, 6441,\n",
              "        1165, 7147, 5341, 6573, 6642, 6607, 2726, 5341, 6573, 6642, 6607,\n",
              "         517, 6896, 6855, 6441, 6909, 6441, 6642, 6607, 4598, 6493, 4446,\n",
              "        6371,  938, 4420, 7798, 4213, 7119, 7178,  980, 6705, 5782, 3175,\n",
              "        5678, 5782, 1229, 2923, 5782, 1504, 5782, 2726, 6896, 6855, 6441,\n",
              "        6909, 6441, 2726, 7476, 6494, 5765, 6642, 6607,    3], dtype=int32),\n",
              " array(64, dtype=int32),\n",
              " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       dtype=int32),\n",
              " 5)"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3stu8Grs7C0",
        "outputId": "1a8a5bd6-414b-4e85-da30-1d41a4b55000"
      },
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
        "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzOtlcArtAvN"
      },
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_size = 768,\n",
        "                 num_classes=10,   ##클래스 수 조정##\n",
        "                 dr_rate=None,\n",
        "                 params=None):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dr_rate = dr_rate\n",
        "                 \n",
        "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
        "        if dr_rate:\n",
        "            self.dropout = nn.Dropout(p=dr_rate)\n",
        "    \n",
        "    def gen_attention_mask(self, token_ids, valid_length):\n",
        "        attention_mask = torch.zeros_like(token_ids)\n",
        "        for i, v in enumerate(valid_length):\n",
        "            attention_mask[i][:v] = 1\n",
        "        return attention_mask.float()\n",
        "\n",
        "    def forward(self, token_ids, valid_length, segment_ids):\n",
        "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
        "        \n",
        "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
        "        if self.dr_rate:\n",
        "            out = self.dropout(pooler)\n",
        "        return self.classifier(out)"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pbo4426rtD65",
        "outputId": "940049c1-de4c-4def-e861-8f773d5f5a37"
      },
      "source": [
        "#BERT 모델 불러오기\n",
        "model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)\n",
        "\n",
        "#optimizer와 schedule 설정\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "t_total = len(train_dataloader) * num_epochs\n",
        "warmup_step = int(t_total * warmup_ratio)\n",
        "\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
        "\n",
        "#정확도 측정을 위한 함수 정의\n",
        "def calc_accuracy(X,Y):\n",
        "    max_vals, max_indices = torch.max(X, 1)\n",
        "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
        "    return train_acc\n",
        "    \n",
        "train_dataloader"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7f3f107b3390>"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQLQgGBhtGpK"
      },
      "source": [
        "for e in range(num_epochs):\n",
        "    train_acc = 0.0\n",
        "    test_acc = 0.0\n",
        "    model.train()\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
        "        optimizer.zero_grad()\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "        loss = loss_fn(out, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        optimizer.step()\n",
        "        scheduler.step()  # Update learning rate schedule\n",
        "        train_acc += calc_accuracy(out, label)\n",
        "        if batch_id % log_interval == 0:\n",
        "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
        "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
        "    \n",
        "    model.eval()\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "        test_acc += calc_accuracy(out, label)\n",
        "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqyLok2atNZL"
      },
      "source": [
        "# ML/DL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGw6n8Gf1X8D"
      },
      "source": [
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "import tensorflow as tf"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQPeWP0Y1fQg"
      },
      "source": [
        "ml_data = topic_table.copy()"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCCIdZWT15o_"
      },
      "source": [
        "ml_content = ml_data.content"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OmwQpZ5360C"
      },
      "source": [
        "ml_data['word_counts'] = np.NaN"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wLHCFzC3QbG"
      },
      "source": [
        "for i in range(len(ml_data)):\n",
        "  ml_data['word_counts'][i] = len(ml_content[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gAWZ9s53_u6"
      },
      "source": [
        "ml_data.topic = ml_data.topic.astype('int')\n",
        "ml_data.word_counts = ml_data.word_counts.astype('int')"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DHL1FSS436i",
        "outputId": "3174494c-9930-4990-ea78-8903ae37f558"
      },
      "source": [
        "ml_data.word_counts.describe()"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    3734.000000\n",
              "mean      228.206749\n",
              "std       125.365094\n",
              "min        12.000000\n",
              "25%       134.000000\n",
              "50%       204.000000\n",
              "75%       299.000000\n",
              "max      1001.000000\n",
              "Name: word_counts, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDnnD_FF4N9B"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.hist(ml_data.word_counts)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4fxWHPW2MKW"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_data = ml_content\n",
        "y_data = ml_data.topic                                                         \n",
        "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.3)"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEis0Qx01lv1"
      },
      "source": [
        "tokenizer=Tokenizer() #토크나이저 \n",
        "tokenizer.fit_on_texts(x_train)\n",
        "train_sequences=tokenizer.texts_to_sequences(x_train)\n",
        "test_sequences=tokenizer.texts_to_sequences(x_test)"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R57Rv-7s2hjI"
      },
      "source": [
        "# 단어 사전 형태\n",
        "word_vocab=tokenizer.word_index"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acg93f-Z2kEg"
      },
      "source": [
        "# 문장 최대 길이\n",
        "MAX_SEQUENCE_LENGTH=250"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93wUoDPj2mpG"
      },
      "source": [
        "# 학습 데이터를 벡터화\n",
        "train_inputs=pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
        "# 학습 데이터의 라벨\n",
        "train_labels=np.array(y_train)\n",
        "\n",
        "# 평가 데이터를 벡터화\n",
        "test_inputs=pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
        "# 평가 데이터의 라벨\n",
        "test_labels=np.array(y_test)"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3dlnPaJ7t-z"
      },
      "source": [
        "vocab_size = len(word_vocab)+1"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjQpVqZR7f9m"
      },
      "source": [
        "# 하이퍼 파라미터\n",
        "VOCAB_SIZE = vocab_size+1\n",
        "EMB_SIZE = 128\n",
        "BATCH_SIZE = 16\n",
        "NUM_EPOCHS = 1"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfSFXjlo8dlM"
      },
      "source": [
        "input_train, input_eval, label_train, label_eval=train_test_split(train_inputs, train_labels, test_size=0.1, random_state=256)"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iDmsJZp74nm"
      },
      "source": [
        "def mapping_fn(X,Y): #입력 함수 정의\n",
        "    input,label={'x':X},Y\n",
        "    return input, label\n",
        "\n",
        "def train_input_fn():\n",
        "    dataset=tf.data.Dataset.from_tensor_slices((input_train,label_train))\n",
        "    dataset=dataset.shuffle(buffer_size=len(input_train))\n",
        "    dataset=dataset.batch(BATCH_SIZE)\n",
        "    dataset=dataset.map(mapping_fn)\n",
        "    dataset=dataset.repeat(count=NUM_EPOCHS)\n",
        "    iterator= iter(dataset)\n",
        "    \n",
        "    return iterator.get_next()\n",
        "\n",
        "def eval_input_fn():\n",
        "    dataset=tf.data.Dataset.from_tensor_slices((input_eval, label_eval))\n",
        "    dataset=dataset.shuffle(buffer_size=len(input_eval))\n",
        "    dataset=dataset.batch(16)\n",
        "    dataset=dataset.map(mapping_fn)\n",
        "    iterator=iter(dataset)\n",
        "    \n",
        "    return iterator.get_next()"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0esr5ID8tcm"
      },
      "source": [
        "def model_fn(features, labels, mode, params):\n",
        "    TRAIN=mode==tf.estimator.ModeKeys.TRAIN\n",
        "    EVAL=mode==tf.estimator.ModeKeys.EVAL\n",
        "    PREDICT=mode==tf.estimator.ModeKeys.PREDICT\n",
        "    \n",
        "    embedding_layer=tf.keras.layers.Embedding(VOCAB_SIZE, EMB_SIZE)(features['x'])\n",
        "    \n",
        "    dropout_emb=tf.keras.layers.Dropout(rate=0.2)(embedding_layer)\n",
        "    \n",
        "    conv=tf.keras.layers.Conv1D(filters=32, kernel_size=3, padding='same', activation=tf.nn.relu)(dropout_emb)\n",
        "    \n",
        "    pool=tf.keras.layers.GlobalMaxPool1D()(conv)\n",
        "    \n",
        "    hidden=tf.keras.layers.Dense(units=250, activation=tf.nn.relu)(pool)\n",
        "    \n",
        "    dropout_hidden=tf.keras.layers.Dropout(rate=0.2)(hidden, training=TRAIN)\n",
        "    logits=tf.keras.layers.Dense(units=1)(dropout_hidden)\n",
        "    \n",
        "    if labels is not None:\n",
        "        labels=tf.reshape(labels,[-1,1])\n",
        "        \n",
        "    if TRAIN:\n",
        "        global_step=tf.train.get_global_step()\n",
        "        loss=tf.losses.categorical_crossentropy(labels,logits)\n",
        "        train_op=tf.train.AdamOptimizer(0.001).minimize(loss, global_step)\n",
        "        \n",
        "        return tf.estimator.EstimatorSpec(mode=mode, train_op=train_op, loss=loss)\n",
        "    \n",
        "    elif EVAL:\n",
        "        loss=tf.losses.categorical_crossentropy(labels,logits)\n",
        "        pred=tf.nn.softmax_cross_entropy_with_logits(logits)\n",
        "        accuracy=tf.metrics.accuracy(labels, tf.round(pred))\n",
        "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops={'acc':accuracy})\n",
        "    \n",
        "    elif PREDICT:\n",
        "        return tf.estimator.EstimatorSpec(mode=mode,predictions={'prob':tf.nn.softmax_cross_entropy_with_logits(logits),})"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtQv7W7C-5aA",
        "outputId": "018e418c-fa63-4ae2-85fe-c903321dd0ad"
      },
      "source": [
        "est=tf.estimator.Estimator(model_fn)"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Using default config.\n",
            "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpqdwf0uvd\n",
            "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpqdwf0uvd', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "WARNING:tensorflow:Estimator's model_fn (<function model_fn at 0x7f3f0805b050>) includes params argument, but params are not passed to Estimator.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_VnxJFs_Wo5"
      },
      "source": [
        "import datetime"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "UtVlVyVZ_Gcu",
        "outputId": "6e6ad9fa-7d1b-4855-c5e4-1ab9f5bcf411"
      },
      "source": [
        "time_start=datetime.datetime.utcnow()\n",
        "print(\"Experiment started at {}\".format(time_start.strftime(\"%H:%M:%S\")))\n",
        "print(\".....................................\")\n",
        "\n",
        "est.train(train_input_fn)"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment started at 14:28:06\n",
            ".....................................\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-159-973ce1e97375>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".....................................\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1174\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m       features, labels, input_hooks = (\n\u001b[0;32m-> 1202\u001b[0;31m           self._get_features_and_labels_from_input_fn(input_fn, ModeKeys.TRAIN))\n\u001b[0m\u001b[1;32m   1203\u001b[0m       \u001b[0mworker_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m       estimator_spec = self._call_model_fn(features, labels, ModeKeys.TRAIN,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_get_features_and_labels_from_input_fn\u001b[0;34m(self, input_fn, mode)\u001b[0m\n\u001b[1;32m   1036\u001b[0m     \u001b[0;34m\"\"\"Extracts the `features` and labels from return values of `input_fn`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m     return estimator_util.parse_input_fn_result(\n\u001b[0;32m-> 1038\u001b[0;31m         self._call_input_fn(input_fn, mode))\n\u001b[0m\u001b[1;32m   1039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extract_batch_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_evaluated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_input_fn\u001b[0;34m(self, input_fn, mode, input_context)\u001b[0m\n\u001b[1;32m   1129\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_context'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/cpu:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-155-cd7334575b94>\u001b[0m in \u001b[0;36mtrain_input_fn\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapping_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0miterator\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n\u001b[0m\u001b[1;32m    414\u001b[0m                          \"or when eager execution is enabled.\")\n\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: __iter__() is only supported inside of tf.function or when eager execution is enabled."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6Fw3KQG_SrR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}