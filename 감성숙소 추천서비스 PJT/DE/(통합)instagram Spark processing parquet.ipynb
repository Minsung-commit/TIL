{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72ca015d",
   "metadata": {},
   "source": [
    "### DEinstagram crawling to csv\n",
    "   \n",
    "- daily_gamsung.csv \n",
    "- gamsung.bnb.csv \n",
    "- gamsung_curation.csv \n",
    "- hi.stay.tour.csv \n",
    "- rest_behappyhere.csv \n",
    "- sookso.diary.csv \n",
    "- sookso.hada.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf3f026",
   "metadata": {},
   "source": [
    "1. 숙소이름 추출  \n",
    "2. 계정별 숙소 리스트 취합  \n",
    "3. 통합 숙소리스트를 기준으로 content 병합  \n",
    "4. like 수 합치기  \n",
    "5. content 일괄 전처리(토큰화, 불용어제거)  \n",
    "\n",
    "'''\n",
    "1. 중복된 이름 검색\n",
    "2. 개수가 1인거 이상 뽑아내기\n",
    "3. 리스트만들기\n",
    "4. 행가져와서 like집계, content, tag 리스트로 append\n",
    "5. 차례대로 기존 행 지워내기\n",
    "6. place 네이버 API 사용해서 대체\n",
    "'''\n",
    "\n",
    "### 최종 데이터 셋  \n",
    "columns = 숙소명, content(통합), like(통합), place(api), tags(통합), 게시수(중복값)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d312252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [content, date, like, tags, imgUrl, name, overlap, place, stage, local]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def daily_gamsung_extract(x):\n",
    "    p = re.compile('\\\"[가-힣\\t\\n\\r\\f\\v\\s\\_가-힣]+\\\"')\n",
    "    m = p.findall(x)\n",
    "    try:\n",
    "        return m[0][1:-1]\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def gamsung_bnb_extract(x):\n",
    "    p = re.compile(\"[\\'|\\\"][가-힣]+[\\'|\\\"]\") \n",
    "    m = p.findall(x)\n",
    "    try:\n",
    "        return m[0][1:-1]\n",
    "    except :\n",
    "        return np.nan\n",
    "\n",
    "def gamsung_curation_extract(x):\n",
    "    p = re.compile('\\📍[가-힣]+') # \n",
    "    m = p.findall(x)\n",
    "    try:\n",
    "        return m[0][1:] \n",
    "    except :\n",
    "        return ''\n",
    "\n",
    "def hi_stay_tour_extract(x):\n",
    "    p = re.compile(\"\\#[가-힣]+\") # \n",
    "    m = p.findall(x)\n",
    "    try:\n",
    "        return m[0][1:] \n",
    "    except :\n",
    "        return ''\n",
    "\n",
    "def rest_behappyhere_extract(x):\n",
    "    p = re.compile('\\#[가-힣]+')\n",
    "    m = p.findall(x)\n",
    "    try:\n",
    "        return m[1][1:]\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def sookso_diary_extract(x):\n",
    "    p = re.compile('\\📍 [가-힣]+\\s[가-힇]+|\\📍[가-힣]+\\s[가-힇]+')\n",
    "    m = p.findall(x)\n",
    "    try:\n",
    "        return m[0].split(' ')[-1]\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def sookso_hada_extract(x):\n",
    "    p = re.compile('\\#[가-힣]+')\n",
    "    m = p.findall(x)\n",
    "    if  m[1] == '#숙소하다': m[1], m[0] = m[0], m[1]\n",
    "    return m[1][1:]\n",
    "\n",
    "\n",
    "def daily_gamsung_extract(x):\n",
    "    p = re.compile('\\\"[가-힣\\t\\n\\r\\f\\v\\s\\_가-힣]+\\\"')\n",
    "    m = p.findall(x)\n",
    "    try:\n",
    "        return m[0][1:-1]\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# def convert(x):\n",
    "#     return int(x.replace(',',''))\n",
    "\n",
    "def convert(x):\n",
    "    if x:\n",
    "        return int(x.replace(',',''))\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "#####################################################\n",
    "### daily_gamsung(완료)    \n",
    "def insta_spark_processing():\n",
    "    from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "    from pyspark.sql.functions import array_contains, udf\n",
    "    from pyspark.conf import SparkConf\n",
    "    from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "    from datetime import datetime, timedelta\n",
    "    from pymongo import MongoClient\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from difflib import SequenceMatcher\n",
    "    from collections import Counter\n",
    "    from datetime import datetime\n",
    "    import re\n",
    "\n",
    "    import time\n",
    "    import os\n",
    "    import sys\n",
    "    import urllib.request\n",
    "    import json\n",
    "\n",
    "    nowtime = datetime.today().strftime(\"%Y-%m-%d\")#  - pd.DateOffset(days=1)\n",
    "    spark = SparkSession\\\n",
    "            .builder\\\n",
    "            .appName('Python Spark SQL basic example')\\\n",
    "            .getOrCreate()\n",
    "\n",
    "    sc = spark.sparkContext\n",
    "    \n",
    "    # pymongo connect\n",
    "    client = MongoClient('localhost',27017) # mongodb 27017 port\n",
    "    db = client.ojo_db\n",
    "    cur = db['cluster']\n",
    "\n",
    "    coronaStage = spark.read.parquet(f\"hdfs://localhost:9000/data/corona/coronaStage_2021-10-04\")\n",
    "\n",
    "    daily_gamsung_DFP = spark.read.parquet(f\"hdfs://localhost:9000/data/insta/daily_gamsung_{nowtime}.parquet\")\n",
    "    daily_gamsung_DFP = daily_gamsung_DFP.drop('Unnamed: 0','unix')\n",
    "    daily_gamsung = daily_gamsung_DFP.toPandas()\n",
    "\n",
    "    gamsung_bnb_DFP = spark.read.parquet(f\"hdfs://localhost:9000/data/insta/gamsung_bnb_{nowtime}.parquet\")\n",
    "    gamsung_bnb_DFP = gamsung_bnb_DFP.drop('Unnamed: 0','unix')\n",
    "    gamsung_bnb = gamsung_bnb_DFP.toPandas()\n",
    "\n",
    "    gamsung_curation_DFP = spark.read.parquet(f\"hdfs://localhost:9000/data/insta/gamsung_curation_{nowtime}.parquet\")\n",
    "    gamsung_curation_DFP = gamsung_curation_DFP.drop('Unnamed: 0','unix')\n",
    "    gamsung_curation = gamsung_curation_DFP.toPandas()\n",
    "\n",
    "    hi_stay_tour_DFP = spark.read.parquet(f\"hdfs://localhost:9000/data/insta/hi_stay_tour_{nowtime}.parquet\")\n",
    "    hi_stay_tour_DFP = hi_stay_tour_DFP.drop('Unnamed: 0','unix')\n",
    "    hi_stay_tour = hi_stay_tour_DFP.toPandas()\n",
    "\n",
    "    rest_behappyhere_DFP = spark.read.parquet(f\"hdfs://localhost:9000/data/insta/rest_behappyhere_{nowtime}.parquet\")\n",
    "    rest_behappyhere_DFP = rest_behappyhere_DFP.drop('Unnamed: 0','unix')\n",
    "    rest_behappyhere = rest_behappyhere_DFP.toPandas()\n",
    "\n",
    "    sookso_diary_DFP = spark.read.parquet(f\"hdfs://localhost:9000/data/insta/sookso_diary_{nowtime}.parquet\")\n",
    "    sookso_diary_DFP = sookso_diary_DFP.drop('Unnamed: 0','unix')\n",
    "    sookso_diary = sookso_diary_DFP.toPandas()\n",
    "\n",
    "    sookso_hada_DFP = spark.read.parquet(f\"hdfs://localhost:9000/data/insta/sookso_hada_{nowtime}.parquet\")\n",
    "    sookso_hada_DFP = sookso_hada_DFP.drop('Unnamed: 0','unix')\n",
    "    sookso_hada = sookso_hada_DFP.toPandas()\n",
    "    \n",
    "    #######################################\n",
    "    daily_gamsung_name = daily_gamsung.content.apply(lambda x : daily_gamsung_extract(x))\n",
    "    daily_gamsung['name'] = daily_gamsung_name\n",
    "    daily_gamsung = daily_gamsung[~(daily_gamsung['name'].isna())]\n",
    "\n",
    "    # name data가 없는 index 지우기\n",
    "    indexNames = daily_gamsung[daily_gamsung['name'] == ''].index\n",
    "    daily_gamsung.drop(indexNames, inplace=True) \n",
    "\n",
    "    if daily_gamsung.like.dtype == 'O':\n",
    "        daily_gamsung.like = daily_gamsung.like.apply(lambda x : convert(x))\n",
    "\n",
    "\n",
    "    # 1,2,3\n",
    "    dg_cnt = Counter(daily_gamsung.name)\n",
    "    li =[]\n",
    "    for name, cnt  in dg_cnt.items():\n",
    "        if cnt > 1:\n",
    "            li.append(name)   \n",
    "\n",
    "    # 4. 행가져와서 like집계, content, tag list append\n",
    "    for i in li:           # 중복된 name 반복\n",
    "        ex = daily_gamsung.loc[daily_gamsung.name == i]  # 반복된 name의 행을 가져온다\n",
    "        ex.reset_index(drop=True,inplace=True)     # index reset\n",
    "\n",
    "        content = ex.iloc[0,:].content\n",
    "        date = ex.iloc[0,:].date\n",
    "        like = ex.iloc[0,:].like\n",
    "        place = ex.iloc[0,:].place\n",
    "        tags = ex.iloc[0,:].tags\n",
    "        imgUrl = ex.iloc[0,:].imgUrl\n",
    "        name = ex.name[0]\n",
    "\n",
    "        for i in range(len(ex)-1):\n",
    "            like += ex.iloc[i+1,:].like\n",
    "            tags += ex.iloc[i+1,:].tags\n",
    "\n",
    "        # 5.차례대로 기존 행 지워내기\n",
    "        indexNames = daily_gamsung[daily_gamsung['name'] == ex.name[0]].index\n",
    "        daily_gamsung.drop(indexNames, inplace=True) \n",
    "        daily_gamsung = daily_gamsung.append(pd.Series(data=[content,date,like,place,tags,imgUrl,name],index=daily_gamsung.columns),ignore_index=True)\n",
    "\n",
    "    dg_tmp = []\n",
    "    for i in daily_gamsung.name:\n",
    "        dg_tmp.append(dg_cnt[i]-1)\n",
    "\n",
    "    daily_gamsung['overlap'] = dg_tmp\n",
    "\n",
    "\n",
    "    #####################################################\n",
    "    ### gamsun.bnb\n",
    "    gamsung_bnb_name = gamsung_bnb.content.apply(lambda x : gamsung_bnb_extract(x)) # gamsung_bnb_name = \n",
    "    gamsung_bnb['name'] = gamsung_bnb_name\n",
    "    gamsung_bnb = gamsung_bnb[~(gamsung_bnb['name'].isna())]\n",
    "\n",
    "    # name data가 없는 index 지우기\n",
    "    indexNames = gamsung_bnb[gamsung_bnb['name'] == ''].index\n",
    "    gamsung_bnb.drop(indexNames, inplace=True) \n",
    "\n",
    "    # like integet 변환 후 합치기\n",
    "\n",
    "    if gamsung_bnb.like.dtype == 'O':\n",
    "        gamsung_bnb.like = gamsung_bnb.like.apply(lambda x : convert(x))\n",
    "\n",
    "\n",
    "\n",
    "    # 1, 중복된 이름 검색 \n",
    "    # 2.개수가 1인거 이상 뽑아내기 \n",
    "    # 3.리스트만들기\n",
    "    gb_cnt = Counter(gamsung_bnb.name)      # Counter 모듈로 name data count\n",
    "    li =[]\n",
    "    for name, cnt  in gb_cnt.items():            # 중복된 것만 뽑기 (cnt가 1이상이면 중복)\n",
    "        if cnt > 1:\n",
    "            li.append(name)\n",
    "\n",
    "    # 4. 행가져와서 like집계, content, tag list append\n",
    "    for i in li:           # 중복된 name 반복\n",
    "        ex = gamsung_bnb.loc[gamsung_bnb.name == i]  # 반복된 name의 행을 가져온다\n",
    "        ex.reset_index(drop=True,inplace=True)     # index reset\n",
    "\n",
    "        content = ex.iloc[0,:].content\n",
    "        date = ex.iloc[0,:].date\n",
    "        like = ex.iloc[0,:].like\n",
    "        place = ex.iloc[0,:].place\n",
    "        tags = ex.iloc[0,:].tags\n",
    "        imgUrl = ex.iloc[0,:].imgUrl\n",
    "        name = ex.name[0]\n",
    "\n",
    "        for i in range(len(ex)-1):\n",
    "            like += ex.iloc[i+1,:].like\n",
    "            tags += ex.iloc[i+1,:].tags\n",
    "\n",
    "\n",
    "        # 5.차례대로 기존 행 지워내기\n",
    "        indexNames = gamsung_bnb[gamsung_bnb['name'] == ex.name[0]].index\n",
    "        gamsung_bnb.drop(indexNames, inplace=True) \n",
    "        gamsung_bnb = gamsung_bnb.append(pd.Series(data=[content,date,like,place,tags,imgUrl,name],index=gamsung_bnb.columns),ignore_index=True)\n",
    "\n",
    "    gb_tmp = []\n",
    "    for i in gamsung_bnb.name:\n",
    "        gb_tmp.append(gb_cnt[i]-1)\n",
    "\n",
    "\n",
    "    gamsung_bnb['overlap'] = gb_tmp\n",
    "\n",
    "\n",
    "    #####################################################\n",
    "    ### gamsung_curation(완료)\n",
    "    gamsung_curation_name = gamsung_curation.content.apply(lambda x : gamsung_curation_extract(x))\n",
    "    gamsung_curation['name'] = gamsung_curation_name\n",
    "    gamsung_curation = gamsung_curation[~(gamsung_curation['name'].isna())]\n",
    "\n",
    "\n",
    "    # name data가 없는 index 지우기\n",
    "    indexNames = gamsung_curation[gamsung_curation['name'] == ''].index\n",
    "    gamsung_curation.drop(indexNames, inplace=True) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if gamsung_curation.like.dtype == 'O':\n",
    "        gamsung_curation.like = gamsung_curation.like.apply(lambda x : convert(x))\n",
    "\n",
    "\n",
    "    gc_cnt = Counter(gamsung_curation.name)      # Counter 모듈로 name data count\n",
    "    li =[]\n",
    "    for name, cnt  in gc_cnt.items():            # 중복된 것만 뽑기 (cnt가 1이상이면 중복)\n",
    "        if cnt > 1:\n",
    "            li.append(name)        \n",
    "\n",
    "\n",
    "    # 4. 행가져와서 like집계, content, tag list append\n",
    "    for i in li:           # 중복된 name 반복\n",
    "        ex = gamsung_curation.loc[gamsung_curation.name == i]  # 반복된 name의 행을 가져온다\n",
    "        ex.reset_index(drop=True,inplace=True)     # index reset\n",
    "\n",
    "        content = ex.iloc[0,:].content\n",
    "        date = ex.iloc[0,:].date\n",
    "        like = ex.iloc[0,:].like\n",
    "        place = ex.iloc[0,:].place\n",
    "        tags = ex.iloc[0,:].tags\n",
    "        imgUrl = ex.iloc[0,:].imgUrl\n",
    "        name = ex.name[0]\n",
    "\n",
    "        for i in range(len(ex)-1):\n",
    "            like += ex.iloc[i+1,:].like\n",
    "            tags += ex.iloc[i+1,:].tags\n",
    "\n",
    "        # 5.차례대로 기존 행 지워내기\n",
    "        indexNames = gamsung_curation[gamsung_curation['name'] == ex.name[0]].index\n",
    "        gamsung_curation.drop(indexNames, inplace=True) \n",
    "        gamsung_curation = gamsung_curation.append(pd.Series(data=[content,date,like,place,tags,imgUrl,name],index=gamsung_curation.columns),ignore_index=True)\n",
    "\n",
    "    gc_tmp = []\n",
    "    for i in gamsung_curation.name:\n",
    "        gc_tmp.append(gc_cnt[i]-1)\n",
    "\n",
    "    gamsung_curation['overlap'] = gc_tmp\n",
    "\n",
    "    #####################################################\n",
    "    ### hi.stay.tour.csv (완료)\n",
    "    hi_stay_tour_name = hi_stay_tour.content.apply(lambda x : hi_stay_tour_extract(x))\n",
    "    hi_stay_tour['name'] = hi_stay_tour_name\n",
    "    hi_stay_tour = hi_stay_tour[~(hi_stay_tour['name'].isna())]\n",
    "\n",
    "\n",
    "    # name data가 없는 index 지우기\n",
    "    indexNames = hi_stay_tour[hi_stay_tour['name'] == ''].index\n",
    "    hi_stay_tour.drop(indexNames, inplace=True) \n",
    "\n",
    "\n",
    "    if hi_stay_tour.like.dtype == 'O':\n",
    "        hi_stay_tour.like = hi_stay_tour.like.apply(lambda x : convert(x))\n",
    "\n",
    "\n",
    "    # 1, 중복된 이름 검색 \n",
    "    # 2.개수가 1인거 이상 뽑아내기 \n",
    "    # 3.리스트만들기\n",
    "    hst_cnt = Counter(hi_stay_tour.name)      # Counter 모듈로 name data count\n",
    "    li =[]\n",
    "    for name, cnt  in hst_cnt.items():            # 중복된 것만 뽑기 (cnt가 1이상이면 중복)\n",
    "        if cnt > 1:\n",
    "            li.append(name)  \n",
    "\n",
    "    # 4. 행가져와서 like집계, content, tag list append\n",
    "    for i in li:           # 중복된 name 반복\n",
    "        ex = hi_stay_tour.loc[hi_stay_tour.name == i]  # 반복된 name의 행을 가져온다\n",
    "        ex.reset_index(drop=True,inplace=True)     # index reset\n",
    "\n",
    "        content = ex.iloc[0,:].content\n",
    "        date = ex.iloc[0,:].date\n",
    "        like = ex.iloc[0,:].like\n",
    "        place = ex.iloc[0,:].place\n",
    "        tags = ex.iloc[0,:].tags\n",
    "        imgUrl = ex.iloc[0,:].imgUrl\n",
    "        name = ex.name[0]\n",
    "\n",
    "        for i in range(len(ex)-1):\n",
    "            like += ex.iloc[i+1,:].like\n",
    "            tags += ex.iloc[i+1,:].tags\n",
    "\n",
    "\n",
    "\n",
    "        # 5.차례대로 기존 행 지워내기\n",
    "        indexNames = hi_stay_tour[hi_stay_tour['name'] == ex.name[0]].index\n",
    "        hi_stay_tour.drop(indexNames, inplace=True) \n",
    "        hi_stay_tour = hi_stay_tour.append(pd.Series(data=[content,date,like,place,tags,imgUrl,name],index=hi_stay_tour.columns),ignore_index=True)\n",
    "\n",
    "    hst_cnt_tmp = []\n",
    "    for i in hi_stay_tour.name:\n",
    "        hst_cnt_tmp.append(hst_cnt[i]-1)\n",
    "\n",
    "    hi_stay_tour['overlap'] = hst_cnt_tmp\n",
    "\n",
    "\n",
    "    #####################################################\n",
    "    ### rest_behappyhere.csv\n",
    "    rest_behappyhere_name = rest_behappyhere.content.apply(lambda x : rest_behappyhere_extract(x))\n",
    "    rest_behappyhere['name'] = rest_behappyhere_name\n",
    "    rest_behappyhere = rest_behappyhere[~(rest_behappyhere['name'].isna())]\n",
    "\n",
    "    # name data가 없는 index 지우기\n",
    "    indexNames = rest_behappyhere[rest_behappyhere['name'] == ''].index\n",
    "    rest_behappyhere.drop(indexNames, inplace=True) \n",
    "\n",
    "\n",
    "\n",
    "    if rest_behappyhere.like.dtype == 'O':\n",
    "        rest_behappyhere.like = rest_behappyhere.like.apply(lambda x : convert(x))\n",
    "\n",
    "\n",
    "    # 1, 중복된 이름 검색 \n",
    "    # 2.개수가 1인거 이상 뽑아내기 \n",
    "    # 3.리스트만들기\n",
    "    rb_cnt = Counter(rest_behappyhere.name)      # Counter 모듈로 name data count\n",
    "    li =[]\n",
    "    for name, cnt  in rb_cnt.items():            # 중복된 것만 뽑기 (cnt가 1이상이면 중복)\n",
    "        if cnt > 1:\n",
    "            li.append(name)\n",
    "\n",
    "\n",
    "    #####################################################\n",
    "    # 4. 행가져와서 like집계, content, tag list append\n",
    "    for i in li:           # 중복된 name 반복\n",
    "        ex = rest_behappyhere.loc[rest_behappyhere.name == i]  # 반복된 name의 행을 가져온다\n",
    "        ex.reset_index(drop=True,inplace=True)     # index reset\n",
    "\n",
    "        content = ex.iloc[0,:].content\n",
    "        date = ex.iloc[0,:].date\n",
    "        like = ex.iloc[0,:].like\n",
    "        place = ex.iloc[0,:].place\n",
    "        tags = ex.iloc[0,:].tags\n",
    "        imgUrl = ex.iloc[0,:].imgUrl\n",
    "        name = ex.name[0]\n",
    "\n",
    "        for i in range(len(ex)-1):\n",
    "            like += ex.iloc[i+1,:].like\n",
    "            tags += ex.iloc[i+1,:].tags\n",
    "\n",
    "\n",
    "\n",
    "        # 5.차례대로 기존 행 지워내기\n",
    "        indexNames = rest_behappyhere[rest_behappyhere['name'] == ex.name[0]].index\n",
    "        rest_behappyhere.drop(indexNames, inplace=True) \n",
    "        rest_behappyhere = rest_behappyhere.append(pd.Series(data=[content,date,like,place,tags,imgUrl,name],index=rest_behappyhere.columns),ignore_index=True)\n",
    "\n",
    "    rb_cnt_tmp = []\n",
    "    for i in rest_behappyhere.name:\n",
    "        rb_cnt_tmp.append(rb_cnt[i]-1)\n",
    "\n",
    "    rest_behappyhere['overlap'] = rb_cnt_tmp\n",
    "\n",
    "    #####################################################\n",
    "    ### sookso.diary(완료)\n",
    "    sookso_diary_name = sookso_diary.content.apply(lambda x : sookso_diary_extract(x))\n",
    "    sookso_diary['name'] = sookso_diary_name\n",
    "    sookso_diary = sookso_diary[~(sookso_diary['name'].isna())]\n",
    "\n",
    "\n",
    "    # name data가 없는 index 지우기\n",
    "    indexNames = sookso_diary[sookso_diary['name'] == ''].index\n",
    "    sookso_diary.drop(indexNames, inplace=True) \n",
    "\n",
    "\n",
    "\n",
    "    if sookso_diary.like.dtype == 'O':\n",
    "        sookso_diary.like = sookso_diary.like.apply(lambda x : convert(x))\n",
    "\n",
    "\n",
    "    # 1, 중복된 이름 검색 \n",
    "    # 2.개수가 1인거 이상 뽑아내기 \n",
    "    # 3.리스트만들기\n",
    "    sd_cnt = Counter(sookso_diary.name)      # Counter 모듈로 name data count\n",
    "    li =[]\n",
    "    for name, cnt  in sd_cnt.items():            # 중복된 것만 뽑기 (cnt가 1이상이면 중복)\n",
    "        if cnt > 1:\n",
    "            li.append(name)\n",
    "\n",
    "    # 4. 행가져와서 like집계, content, tag list append\n",
    "    for i in li:           # 중복된 name 반복\n",
    "        ex = sookso_diary.loc[sookso_diary.name == i]  # 반복된 name의 행을 가져온다\n",
    "        ex.reset_index(drop=True,inplace=True)     # index reset\n",
    "\n",
    "        content = ex.iloc[0,:].content\n",
    "        date = ex.iloc[0,:].date\n",
    "        like = ex.iloc[0,:].like\n",
    "        place = ex.iloc[0,:].place\n",
    "        tags = ex.iloc[0,:].tags\n",
    "        imgUrl = ex.iloc[0,:].imgUrl\n",
    "        name = ex.name[0]\n",
    "\n",
    "        for i in range(len(ex)-1):\n",
    "            like += ex.iloc[i+1,:].like\n",
    "            tags += ex.iloc[i+1,:].tags\n",
    "\n",
    "\n",
    "\n",
    "        # 5.차례대로 기존 행 지워내기\n",
    "        indexNames = sookso_diary[sookso_diary['name'] == ex.name[0]].index\n",
    "        sookso_diary.drop(indexNames, inplace=True) \n",
    "        sookso_diary = sookso_diary.append(pd.Series(data=[content,date,like,place,tags,imgUrl,name],index=sookso_diary.columns),ignore_index=True)\n",
    "\n",
    "    sd_cnt_tmp = []\n",
    "    for i in sookso_diary.name:\n",
    "        sd_cnt_tmp.append(sd_cnt[i]-1)\n",
    "\n",
    "    sookso_diary['overlap'] = sd_cnt_tmp\n",
    "\n",
    "    #####################################################\n",
    "\n",
    "    ### sookso.hada (완료)\n",
    "    sookso_hada_name = sookso_hada.content.apply(lambda x : sookso_hada_extract(x))\n",
    "    sookso_hada['name'] = sookso_hada_name\n",
    "    sookso_hada = sookso_hada[~(sookso_hada['name'].isna())]\n",
    "\n",
    "    # name data가 없는 index 지우기\n",
    "    indexNames = sookso_hada[sookso_hada['name'] == ''].index\n",
    "    sookso_hada.drop(indexNames, inplace=True) \n",
    "\n",
    "\n",
    "    # like integet 변환 후 합치기\n",
    "\n",
    "    if sookso_hada.like.dtype == 'O':\n",
    "        sookso_hada.like = sookso_hada.like.apply(lambda x : convert(x))\n",
    "\n",
    "\n",
    "    # 1, 중복된 이름 검색 \n",
    "    # 2.개수가 1인거 이상 뽑아내기 \n",
    "    # 3.리스트만들기\n",
    "    sh_cnt = Counter(sookso_hada.name)      # Counter 모듈로 name data count\n",
    "    li =[]\n",
    "    for name, cnt  in sh_cnt.items():            # 중복된 것만 뽑기 (cnt가 1이상이면 중복)\n",
    "        if cnt > 1:\n",
    "            li.append(name)        \n",
    "\n",
    "\n",
    "    # 4. 행가져와서 like집계, content, tag list append\n",
    "    for i in li:           # 중복된 name 반복\n",
    "        ex = sookso_hada.loc[sookso_hada.name == i]  # 반복된 name의 행을 가져온다\n",
    "        ex.reset_index(drop=True,inplace=True)     # index reset\n",
    "\n",
    "        content = ex.iloc[0,:].content\n",
    "        date = ex.iloc[0,:].date\n",
    "        like = ex.iloc[0,:].like\n",
    "        place = ex.iloc[0,:].place\n",
    "        tags = ex.iloc[0,:].tags\n",
    "        imgUrl = ex.iloc[0,:].imgUrl\n",
    "        name = ex.name[0]\n",
    "\n",
    "        for i in range(len(ex)-1):\n",
    "            like += ex.iloc[i+1,:].like\n",
    "            tags += ex.iloc[i+1,:].tags\n",
    "\n",
    "        # 5.차례대로 기존 행 지워내기\n",
    "        indexNames = sookso_hada[sookso_hada['name'] == ex.name[0]].index\n",
    "        sookso_hada.drop(indexNames, inplace=True) \n",
    "        sookso_hada = sookso_hada.append(pd.Series(data=[content,date,like,place,tags,imgUrl,name],index=sookso_hada.columns),ignore_index=True)\n",
    "\n",
    "    sh_cnt_tmp = []\n",
    "    for i in sookso_hada.name:\n",
    "        sh_cnt_tmp.append(sh_cnt[i]-1)\n",
    "\n",
    "    sookso_hada['overlap'] = sh_cnt_tmp\n",
    "\n",
    "    ######################################\n",
    "    ##  데이터 프레임 합치기\n",
    "    tot_dataset = daily_gamsung\n",
    "    for df in [gamsung_bnb, gamsung_curation, hi_stay_tour, rest_behappyhere, sookso_diary, sookso_hada]:\n",
    "        tot_dataset = tot_dataset.append(df)\n",
    "\n",
    "    if tot_dataset.like.dtype == 'O':\n",
    "        tot_dataset.like = tot_dataset.like.apply(lambda x : convert(x))\n",
    "\n",
    "\n",
    "\n",
    "    dg_cnt = Counter(tot_dataset.name)      # Counter 모듈로 name data count\n",
    "    li =[]\n",
    "    for name, cnt  in dg_cnt.items():            # 중복된 것만 뽑기 (cnt가 1이상이면 중복)\n",
    "        if cnt > 1:\n",
    "            li.append(name)\n",
    "\n",
    "\n",
    "    # 4. 행가져와서 like집계, content, tag list append\n",
    "    for i in li:           # 중복된 name 반복\n",
    "        ex = tot_dataset.loc[tot_dataset.name == i]  # 반복된 name의 행을 가져온다\n",
    "        ex.reset_index(drop=True,inplace=True)     # index reset\n",
    "\n",
    "        content = ex.iloc[0,:].content\n",
    "        date = ex.iloc[0,:].date\n",
    "        like = ex.iloc[0,:].like\n",
    "        place = ex.iloc[0,:].place\n",
    "        tags = ex.iloc[0,:].tags\n",
    "        imgUrl = ex.iloc[0,:].imgUrl\n",
    "        name = ex.name[0]\n",
    "        overlap = ex.iloc[0,:].overlap\n",
    "\n",
    "        for i in range(len(ex)-1):\n",
    "            like += ex.iloc[i+1,:].like\n",
    "            tags += ex.iloc[i+1,:].tags\n",
    "            overlap += ex.iloc[i+1,:].overlap\n",
    "\n",
    "\n",
    "        # 5.차례대로 기존 행 지워내기\n",
    "        indexNames = tot_dataset[tot_dataset['name'] == ex.name[0]].index\n",
    "        tot_dataset.drop(indexNames, inplace=True) \n",
    "        tot_dataset = tot_dataset.append(pd.Series(data=[content,date,like,place,tags,imgUrl,name,overlap],index=tot_dataset.columns),ignore_index=True)\n",
    "\n",
    "    ####################################\n",
    "    ## Naver 장소 API\n",
    "\n",
    "    # naver application clientID,secret\n",
    "    client_id = \"6E6mBjvxbdEgVk6Prqgd\"\n",
    "    client_secret = \"0ZjWnYiBjr\"\n",
    "\n",
    "\n",
    "    # quote가 인코딩을 변경(UTF8로 url에 넘겨주기 위해서)\n",
    "    # 내가 입력한 값을 기준으로 parse\n",
    "    title_et = []\n",
    "    for name in tot_dataset.name:\n",
    "        query = urllib.parse.quote(f'숙소+\" \"+{name}')\n",
    "        idx = 0 # 요청 개수\n",
    "        display = 10 # 100개 단위로 가져온다\n",
    "        start = 1\n",
    "        end = 100\n",
    "        tmp=''\n",
    "\n",
    "        url = \"https://openapi.naver.com/v1/search/local?query=\" + query \\\n",
    "            + \"&display=\" + str(display) \\\n",
    "            + \"&start=\" + str(start)\n",
    "\n",
    "\n",
    "        request = urllib.request.Request(url)\n",
    "        request.add_header(\"X-Naver-Client-Id\",client_id)\n",
    "        request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
    "        # request 요청에 대한 결과를 response에 저장\n",
    "        response = urllib.request.urlopen(request)\n",
    "        rescode = response.getcode()\n",
    "        if(rescode==200):\n",
    "            response_body = response.read()\n",
    "            response_dict = json.loads(response_body.decode('utf-8'))\n",
    "            items = response_dict['items']\n",
    "            for item_index in range(0,len(items)):\n",
    "                remove_tag = re.compile('<.*?>')\n",
    "                title = re.sub(remove_tag, '', items[item_index]['title'])\n",
    "                link = items[item_index]['link']\n",
    "                caterory = items[item_index]['category']\n",
    "                description = re.sub(remove_tag, '',items[item_index]['description'])\n",
    "                telephone = items[item_index]['telephone']\n",
    "                address = items[item_index]['address']\n",
    "                roadAddress = items[item_index]['roadAddress']\n",
    "                mapx = items[item_index]['mapx']\n",
    "                mapy = items[item_index]['mapy']\n",
    "\n",
    "                # 이름이 없는 경우\n",
    "                if not title :\n",
    "                    title_et.append('')\n",
    "                else: #'전통숙소' '호스텔''리조트''리조트부속건물'\n",
    "                    if caterory[:2] in ['숙박'] :  # 카테고리가 숙박인 경우\n",
    "                        title = title.replace(' ','')\n",
    "                        name = name.replace(' ','')\n",
    "                        ratio = SequenceMatcher(None, title, name).ratio()\n",
    "                        if ratio >= 0.7 : # title.replace(' ','') in name.replace(' ','')\n",
    "                            tmp = address\n",
    "\n",
    "            if tmp : title_et.append(tmp)\n",
    "            else: title_et.append('')\n",
    "\n",
    "        else:\n",
    "            print(\"Error Code:\" + rescode)\n",
    "\n",
    "        time.sleep(0.1)\n",
    "\n",
    "\n",
    "    tot_dataset.drop('place',axis=1,inplace=True)\n",
    "    tot_dataset['place'] = title_et\n",
    "\n",
    "    #####################################\n",
    "    ### 코로나 단계 작업\n",
    "    coronaStage = coronaStage.toPandas() \n",
    "\n",
    "    stage =[str() for i in range(len(tot_dataset))]\n",
    "    local = [str() for i in range(len(tot_dataset))]\n",
    "    for lo in coronaStage.Area.unique().tolist():\n",
    "        for j in range(len(tot_dataset)):\n",
    "            try :\n",
    "                com = tot_dataset.iloc[j].place.split()[0]\n",
    "                if list(lo)[0] in com and list(lo)[1] in com:\n",
    "                    stage[j] = coronaStage[coronaStage.Area == lo].Stage.values[0]\n",
    "                    local[j] = lo\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    tot_dataset['stage'] = stage\n",
    "    tot_dataset['local'] = local\n",
    "\n",
    "    ############################\n",
    "    ## 새로운 숙소 생성\n",
    "\n",
    "\n",
    "    ch = []\n",
    "    for i in cur.find():\n",
    "        ch.append(i)\n",
    "    change = pd.DataFrame(ch)\n",
    "\n",
    "    a = tot_dataset.merge(change,how='left' ,on='name')\n",
    "    li = a[a.tags_y.isnull() == True].index.tolist()\n",
    "    new = tot_dataset.iloc[li]\n",
    "    print(new)\n",
    "#     new.drop('index',axis=1,inplace=True)\n",
    "    new.to_parquet(f'new_{nowtime}.parquet')\n",
    "\n",
    "insta_spark_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "breathing-slovenia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "applicable-myanmar",
   "metadata": {},
   "outputs": [],
   "source": [
    "change['imgUrl'] = a['imgUrl_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "brief-chapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "change.drop('index',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "electrical-maryland",
   "metadata": {},
   "outputs": [],
   "source": [
    "change.to_parquet(f'tot_dataset_{nowtime}.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-placement",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honest-purple",
   "metadata": {},
   "source": [
    "### 클러스터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a5c4e69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "insta_data = tot_dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "genetic-analyst",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:4: DeprecationWarning: `prepare()` has no effect and will be removed in future version.\n"
     ]
    }
   ],
   "source": [
    "from kiwipiepy import Kiwi\n",
    "\n",
    "kiwi = Kiwi()\n",
    "kiwi.prepare()\n",
    "\n",
    "location = pd.read_csv('/home/ubuntu/DS/location_words.csv')\n",
    "location.columns = ['index', 'location']\n",
    "location = location.location.tolist()\n",
    "\n",
    "def sub_special(s):\n",
    "    rs = re.sub(r'[^가-힣]',' ',s)\n",
    "    rr = re.sub(' +', ' ', rs)\n",
    "    return rr\n",
    "\n",
    "def tokenize_nouns(sent):\n",
    "    res, score = kiwi.analyze(sent)[0] # 첫번째 결과를 사용\n",
    "    return [word + ('다' if tag.startswith('V') else '') # 동사에는 '다'를 붙여줌\n",
    "            for word, tag, _, _ in res\n",
    "            if tag.startswith('N')] # 조사, 어미, 특수기호 및 stopwords에 포함된 단어는 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5b1ff8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n",
      "/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# 1차 클린징(영어, 특수문자, 숫자제거)\n",
    "for i in range(len(insta_data.content)):\n",
    "    insta_data.content[i] = sub_special(insta_data.content[i])\n",
    "\n",
    "# 토큰화\n",
    "for i in range(len(insta_data.content)):\n",
    "    insta_data.content[i]= tokenize_nouns(insta_data.content[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d23ba53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "insta_data['location'] = np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b1bd1693",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = '감성,인,수,등,나,애정,상황,문의,작동,금,동,속,천장,석양,해,해안,남,추천,방송,우정,소원,동일,복,수,내,안전,석,가능,가치,동리,회,사방,궁,당,대,동막,사진,통,창,감,구성,점,시,전,조리,요리,조리,노천,온수,영지,길,장,후,입,시,원,가구,내부,오'\n",
    "stop_words = stop_words.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6ce0ea73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(insta_data.content)):\n",
    "    result = []\n",
    "    for w in insta_data.content[i]:\n",
    "        if w in location and w not in stop_words:\n",
    "            result.append(w)\n",
    "        insta_data.location[i] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "47874fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(insta_data.location)):\n",
    "    temp = set(insta_data.location[i])\n",
    "    insta_data.location[i] = list(temp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8bb61e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          [밀양, 밀양시, 산외면]\n",
       "1                    [여수]\n",
       "2      [화도면, 강화군, 강화, 인천]\n",
       "3          [제주시, 구좌읍, 제주]\n",
       "4           [전북, 전주, 전주시]\n",
       "              ...        \n",
       "694    [상관면, 완주, 완주군, 전주]\n",
       "695        [강릉시, 강릉, 연곡면]\n",
       "696    [중앙, 강릉시, 강릉, 사천면]\n",
       "697        [포항, 포항시, 청하면]\n",
       "698         [정방, 서귀포, 제주]\n",
       "Name: location, Length: 699, dtype: object"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insta_data.location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fea4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "# pymongo connect\n",
    "client = MongoClient('localhost',27017) # mongodb 27017 port\n",
    "db = client.ojo_db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3995c110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # mognodb에 넣기위한 json 형태 변환 후 mongodb collection(corona)로 적재\n",
    "# for k in range(len(tot_dataset)):\n",
    "#     tmp ={}\n",
    "#     for key,value in zip(tot_dataset.columns,tot_dataset.values[k]):\n",
    "#         tmp[key] = value\n",
    "#     db.insta_tot.insert_one(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2395f41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_python3)",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
