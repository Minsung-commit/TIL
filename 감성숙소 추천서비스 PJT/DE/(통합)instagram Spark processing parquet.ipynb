{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72ca015d",
   "metadata": {},
   "source": [
    "### DEinstagram crawling to csv\n",
    "   \n",
    "- daily_gamsung.csv \n",
    "- gamsung.bnb.csv \n",
    "- gamsung_curation.csv \n",
    "- hi.stay.tour.csv \n",
    "- rest_behappyhere.csv \n",
    "- sookso.diary.csv \n",
    "- sookso.hada.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf3f026",
   "metadata": {},
   "source": [
    "1. ìˆ™ì†Œì´ë¦„ ì¶”ì¶œ  \n",
    "2. ê³„ì •ë³„ ìˆ™ì†Œ ë¦¬ìŠ¤íŠ¸ ì·¨í•©  \n",
    "3. í†µí•© ìˆ™ì†Œë¦¬ìŠ¤íŠ¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ content ë³‘í•©  \n",
    "4. like ìˆ˜ í•©ì¹˜ê¸°  \n",
    "5. content ì¼ê´„ ì „ì²˜ë¦¬(í† í°í™”, ë¶ˆìš©ì–´ì œê±°)  \n",
    "\n",
    "'''\n",
    "1. ì¤‘ë³µëœ ì´ë¦„ ê²€ìƒ‰\n",
    "2. ê°œìˆ˜ê°€ 1ì¸ê±° ì´ìƒ ë½‘ì•„ë‚´ê¸°\n",
    "3. ë¦¬ìŠ¤íŠ¸ë§Œë“¤ê¸°\n",
    "4. í–‰ê°€ì ¸ì™€ì„œ likeì§‘ê³„, content, tag ë¦¬ìŠ¤íŠ¸ë¡œ append\n",
    "5. ì°¨ë¡€ëŒ€ë¡œ ê¸°ì¡´ í–‰ ì§€ì›Œë‚´ê¸°\n",
    "6. place ë„¤ì´ë²„ API ì‚¬ìš©í•´ì„œ ëŒ€ì²´\n",
    "'''\n",
    "\n",
    "### ìµœì¢… ë°ì´í„° ì…‹  \n",
    "columns = ìˆ™ì†Œëª…, content(í†µí•©), like(í†µí•©), place(api), tags(í†µí•©), ê²Œì‹œìˆ˜(ì¤‘ë³µê°’)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d312252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [content, date, like, tags, imgUrl, name, overlap, place, stage, local]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def daily_gamsung_extract(x):\n",
    "    p = re.compile('\\\"[ê°€-í£\\t\\n\\r\\f\\v\\s\\_ê°€-í£]+\\\"')\n",
    "    m = p.findall(x)\n",
    "    try:\n",
    "        return m[0][1:-1]\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def gamsung_bnb_extract(x):\n",
    "    p = re.compile(\"[\\'|\\\"][ê°€-í£]+[\\'|\\\"]\") \n",
    "    m = p.findall(x)\n",
    "    try:\n",
    "        return m[0][1:-1]\n",
    "    except :\n",
    "        return np.nan\n",
    "\n",
    "def gamsung_curation_extract(x):\n",
    "    p = re.compile('\\ğŸ“[ê°€-í£]+') # \n",
    "    m = p.findall(x)\n",
    "    try:\n",
    "        return m[0][1:] \n",
    "    except :\n",
    "        return ''\n",
    "\n",
    "def hi_stay_tour_extract(x):\n",
    "    p = re.compile(\"\\#[ê°€-í£]+\") # \n",
    "    m = p.findall(x)\n",
    "    try:\n",
    "        return m[0][1:] \n",
    "    except :\n",
    "        return ''\n",
    "\n",
    "def rest_behappyhere_extract(x):\n",
    "    p = re.compile('\\#[ê°€-í£]+')\n",
    "    m = p.findall(x)\n",
    "    try:\n",
    "        return m[1][1:]\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def sookso_diary_extract(x):\n",
    "    p = re.compile('\\ğŸ“ [ê°€-í£]+\\s[ê°€-í‡]+|\\ğŸ“[ê°€-í£]+\\s[ê°€-í‡]+')\n",
    "    m = p.findall(x)\n",
    "    try:\n",
    "        return m[0].split(' ')[-1]\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def sookso_hada_extract(x):\n",
    "    p = re.compile('\\#[ê°€-í£]+')\n",
    "    m = p.findall(x)\n",
    "    if  m[1] == '#ìˆ™ì†Œí•˜ë‹¤': m[1], m[0] = m[0], m[1]\n",
    "    return m[1][1:]\n",
    "\n",
    "\n",
    "def daily_gamsung_extract(x):\n",
    "    p = re.compile('\\\"[ê°€-í£\\t\\n\\r\\f\\v\\s\\_ê°€-í£]+\\\"')\n",
    "    m = p.findall(x)\n",
    "    try:\n",
    "        return m[0][1:-1]\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# def convert(x):\n",
    "#     return int(x.replace(',',''))\n",
    "\n",
    "def convert(x):\n",
    "    if x:\n",
    "        return int(x.replace(',',''))\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "#####################################################\n",
    "### daily_gamsung(ì™„ë£Œ)    \n",
    "def insta_spark_processing():\n",
    "    from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "    from pyspark.sql.functions import array_contains, udf\n",
    "    from pyspark.conf import SparkConf\n",
    "    from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "    from datetime import datetime, timedelta\n",
    "    from pymongo import MongoClient\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from difflib import SequenceMatcher\n",
    "    from collections import Counter\n",
    "    from datetime import datetime\n",
    "    import re\n",
    "\n",
    "    import time\n",
    "    import os\n",
    "    import sys\n",
    "    import urllib.request\n",
    "    import json\n",
    "\n",
    "    nowtime = datetime.today().strftime(\"%Y-%m-%d\")#  - pd.DateOffset(days=1)\n",
    "    spark = SparkSession\\\n",
    "            .builder\\\n",
    "            .appName('Python Spark SQL basic example')\\\n",
    "            .getOrCreate()\n",
    "\n",
    "    sc = spark.sparkContext\n",
    "    \n",
    "    # pymongo connect\n",
    "    client = MongoClient('localhost',27017) # mongodb 27017 port\n",
    "    db = client.ojo_db\n",
    "    cur = db['cluster']\n",
    "\n",
    "    coronaStage = spark.read.parquet(f\"hdfs://localhost:9000/data/corona/coronaStage_2021-10-04\")\n",
    "\n",
    "    daily_gamsung_DFP = spark.read.parquet(f\"hdfs://localhost:9000/data/insta/daily_gamsung_{nowtime}.parquet\")\n",
    "    daily_gamsung_DFP = daily_gamsung_DFP.drop('Unnamed: 0','unix')\n",
    "    daily_gamsung = daily_gamsung_DFP.toPandas()\n",
    "\n",
    "    gamsung_bnb_DFP = spark.read.parquet(f\"hdfs://localhost:9000/data/insta/gamsung_bnb_{nowtime}.parquet\")\n",
    "    gamsung_bnb_DFP = gamsung_bnb_DFP.drop('Unnamed: 0','unix')\n",
    "    gamsung_bnb = gamsung_bnb_DFP.toPandas()\n",
    "\n",
    "    gamsung_curation_DFP = spark.read.parquet(f\"hdfs://localhost:9000/data/insta/gamsung_curation_{nowtime}.parquet\")\n",
    "    gamsung_curation_DFP = gamsung_curation_DFP.drop('Unnamed: 0','unix')\n",
    "    gamsung_curation = gamsung_curation_DFP.toPandas()\n",
    "\n",
    "    hi_stay_tour_DFP = spark.read.parquet(f\"hdfs://localhost:9000/data/insta/hi_stay_tour_{nowtime}.parquet\")\n",
    "    hi_stay_tour_DFP = hi_stay_tour_DFP.drop('Unnamed: 0','unix')\n",
    "    hi_stay_tour = hi_stay_tour_DFP.toPandas()\n",
    "\n",
    "    rest_behappyhere_DFP = spark.read.parquet(f\"hdfs://localhost:9000/data/insta/rest_behappyhere_{nowtime}.parquet\")\n",
    "    rest_behappyhere_DFP = rest_behappyhere_DFP.drop('Unnamed: 0','unix')\n",
    "    rest_behappyhere = rest_behappyhere_DFP.toPandas()\n",
    "\n",
    "    sookso_diary_DFP = spark.read.parquet(f\"hdfs://localhost:9000/data/insta/sookso_diary_{nowtime}.parquet\")\n",
    "    sookso_diary_DFP = sookso_diary_DFP.drop('Unnamed: 0','unix')\n",
    "    sookso_diary = sookso_diary_DFP.toPandas()\n",
    "\n",
    "    sookso_hada_DFP = spark.read.parquet(f\"hdfs://localhost:9000/data/insta/sookso_hada_{nowtime}.parquet\")\n",
    "    sookso_hada_DFP = sookso_hada_DFP.drop('Unnamed: 0','unix')\n",
    "    sookso_hada = sookso_hada_DFP.toPandas()\n",
    "    \n",
    "    #######################################\n",
    "    daily_gamsung_name = daily_gamsung.content.apply(lambda x : daily_gamsung_extract(x))\n",
    "    daily_gamsung['name'] = daily_gamsung_name\n",
    "    daily_gamsung = daily_gamsung[~(daily_gamsung['name'].isna())]\n",
    "\n",
    "    # name dataê°€ ì—†ëŠ” index ì§€ìš°ê¸°\n",
    "    indexNames = daily_gamsung[daily_gamsung['name'] == ''].index\n",
    "    daily_gamsung.drop(indexNames, inplace=True) \n",
    "\n",
    "    if daily_gamsung.like.dtype == 'O':\n",
    "        daily_gamsung.like = daily_gamsung.like.apply(lambda x : convert(x))\n",
    "\n",
    "\n",
    "    # 1,2,3\n",
    "    dg_cnt = Counter(daily_gamsung.name)\n",
    "    li =[]\n",
    "    for name, cnt  in dg_cnt.items():\n",
    "        if cnt > 1:\n",
    "            li.append(name)   \n",
    "\n",
    "    # 4. í–‰ê°€ì ¸ì™€ì„œ likeì§‘ê³„, content, tag list append\n",
    "    for i in li:           # ì¤‘ë³µëœ name ë°˜ë³µ\n",
    "        ex = daily_gamsung.loc[daily_gamsung.name == i]  # ë°˜ë³µëœ nameì˜ í–‰ì„ ê°€ì ¸ì˜¨ë‹¤\n",
    "        ex.reset_index(drop=True,inplace=True)     # index reset\n",
    "\n",
    "        content = ex.iloc[0,:].content\n",
    "        date = ex.iloc[0,:].date\n",
    "        like = ex.iloc[0,:].like\n",
    "        place = ex.iloc[0,:].place\n",
    "        tags = ex.iloc[0,:].tags\n",
    "        imgUrl = ex.iloc[0,:].imgUrl\n",
    "        name = ex.name[0]\n",
    "\n",
    "        for i in range(len(ex)-1):\n",
    "            like += ex.iloc[i+1,:].like\n",
    "            tags += ex.iloc[i+1,:].tags\n",
    "\n",
    "        # 5.ì°¨ë¡€ëŒ€ë¡œ ê¸°ì¡´ í–‰ ì§€ì›Œë‚´ê¸°\n",
    "        indexNames = daily_gamsung[daily_gamsung['name'] == ex.name[0]].index\n",
    "        daily_gamsung.drop(indexNames, inplace=True) \n",
    "        daily_gamsung = daily_gamsung.append(pd.Series(data=[content,date,like,place,tags,imgUrl,name],index=daily_gamsung.columns),ignore_index=True)\n",
    "\n",
    "    dg_tmp = []\n",
    "    for i in daily_gamsung.name:\n",
    "        dg_tmp.append(dg_cnt[i]-1)\n",
    "\n",
    "    daily_gamsung['overlap'] = dg_tmp\n",
    "\n",
    "\n",
    "    #####################################################\n",
    "    ### gamsun.bnb\n",
    "    gamsung_bnb_name = gamsung_bnb.content.apply(lambda x : gamsung_bnb_extract(x)) # gamsung_bnb_name = \n",
    "    gamsung_bnb['name'] = gamsung_bnb_name\n",
    "    gamsung_bnb = gamsung_bnb[~(gamsung_bnb['name'].isna())]\n",
    "\n",
    "    # name dataê°€ ì—†ëŠ” index ì§€ìš°ê¸°\n",
    "    indexNames = gamsung_bnb[gamsung_bnb['name'] == ''].index\n",
    "    gamsung_bnb.drop(indexNames, inplace=True) \n",
    "\n",
    "    # like integet ë³€í™˜ í›„ í•©ì¹˜ê¸°\n",
    "\n",
    "    if gamsung_bnb.like.dtype == 'O':\n",
    "        gamsung_bnb.like = gamsung_bnb.like.apply(lambda x : convert(x))\n",
    "\n",
    "\n",
    "\n",
    "    # 1, ì¤‘ë³µëœ ì´ë¦„ ê²€ìƒ‰ \n",
    "    # 2.ê°œìˆ˜ê°€ 1ì¸ê±° ì´ìƒ ë½‘ì•„ë‚´ê¸° \n",
    "    # 3.ë¦¬ìŠ¤íŠ¸ë§Œë“¤ê¸°\n",
    "    gb_cnt = Counter(gamsung_bnb.name)      # Counter ëª¨ë“ˆë¡œ name data count\n",
    "    li =[]\n",
    "    for name, cnt  in gb_cnt.items():            # ì¤‘ë³µëœ ê²ƒë§Œ ë½‘ê¸° (cntê°€ 1ì´ìƒì´ë©´ ì¤‘ë³µ)\n",
    "        if cnt > 1:\n",
    "            li.append(name)\n",
    "\n",
    "    # 4. í–‰ê°€ì ¸ì™€ì„œ likeì§‘ê³„, content, tag list append\n",
    "    for i in li:           # ì¤‘ë³µëœ name ë°˜ë³µ\n",
    "        ex = gamsung_bnb.loc[gamsung_bnb.name == i]  # ë°˜ë³µëœ nameì˜ í–‰ì„ ê°€ì ¸ì˜¨ë‹¤\n",
    "        ex.reset_index(drop=True,inplace=True)     # index reset\n",
    "\n",
    "        content = ex.iloc[0,:].content\n",
    "        date = ex.iloc[0,:].date\n",
    "        like = ex.iloc[0,:].like\n",
    "        place = ex.iloc[0,:].place\n",
    "        tags = ex.iloc[0,:].tags\n",
    "        imgUrl = ex.iloc[0,:].imgUrl\n",
    "        name = ex.name[0]\n",
    "\n",
    "        for i in range(len(ex)-1):\n",
    "            like += ex.iloc[i+1,:].like\n",
    "            tags += ex.iloc[i+1,:].tags\n",
    "\n",
    "\n",
    "        # 5.ì°¨ë¡€ëŒ€ë¡œ ê¸°ì¡´ í–‰ ì§€ì›Œë‚´ê¸°\n",
    "        indexNames = gamsung_bnb[gamsung_bnb['name'] == ex.name[0]].index\n",
    "        gamsung_bnb.drop(indexNames, inplace=True) \n",
    "        gamsung_bnb = gamsung_bnb.append(pd.Series(data=[content,date,like,place,tags,imgUrl,name],index=gamsung_bnb.columns),ignore_index=True)\n",
    "\n",
    "    gb_tmp = []\n",
    "    for i in gamsung_bnb.name:\n",
    "        gb_tmp.append(gb_cnt[i]-1)\n",
    "\n",
    "\n",
    "    gamsung_bnb['overlap'] = gb_tmp\n",
    "\n",
    "\n",
    "    #####################################################\n",
    "    ### gamsung_curation(ì™„ë£Œ)\n",
    "    gamsung_curation_name = gamsung_curation.content.apply(lambda x : gamsung_curation_extract(x))\n",
    "    gamsung_curation['name'] = gamsung_curation_name\n",
    "    gamsung_curation = gamsung_curation[~(gamsung_curation['name'].isna())]\n",
    "\n",
    "\n",
    "    # name dataê°€ ì—†ëŠ” index ì§€ìš°ê¸°\n",
    "    indexNames = gamsung_curation[gamsung_curation['name'] == ''].index\n",
    "    gamsung_curation.drop(indexNames, inplace=True) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if gamsung_curation.like.dtype == 'O':\n",
    "        gamsung_curation.like = gamsung_curation.like.apply(lambda x : convert(x))\n",
    "\n",
    "\n",
    "    gc_cnt = Counter(gamsung_curation.name)      # Counter ëª¨ë“ˆë¡œ name data count\n",
    "    li =[]\n",
    "    for name, cnt  in gc_cnt.items():            # ì¤‘ë³µëœ ê²ƒë§Œ ë½‘ê¸° (cntê°€ 1ì´ìƒì´ë©´ ì¤‘ë³µ)\n",
    "        if cnt > 1:\n",
    "            li.append(name)        \n",
    "\n",
    "\n",
    "    # 4. í–‰ê°€ì ¸ì™€ì„œ likeì§‘ê³„, content, tag list append\n",
    "    for i in li:           # ì¤‘ë³µëœ name ë°˜ë³µ\n",
    "        ex = gamsung_curation.loc[gamsung_curation.name == i]  # ë°˜ë³µëœ nameì˜ í–‰ì„ ê°€ì ¸ì˜¨ë‹¤\n",
    "        ex.reset_index(drop=True,inplace=True)     # index reset\n",
    "\n",
    "        content = ex.iloc[0,:].content\n",
    "        date = ex.iloc[0,:].date\n",
    "        like = ex.iloc[0,:].like\n",
    "        place = ex.iloc[0,:].place\n",
    "        tags = ex.iloc[0,:].tags\n",
    "        imgUrl = ex.iloc[0,:].imgUrl\n",
    "        name = ex.name[0]\n",
    "\n",
    "        for i in range(len(ex)-1):\n",
    "            like += ex.iloc[i+1,:].like\n",
    "            tags += ex.iloc[i+1,:].tags\n",
    "\n",
    "        # 5.ì°¨ë¡€ëŒ€ë¡œ ê¸°ì¡´ í–‰ ì§€ì›Œë‚´ê¸°\n",
    "        indexNames = gamsung_curation[gamsung_curation['name'] == ex.name[0]].index\n",
    "        gamsung_curation.drop(indexNames, inplace=True) \n",
    "        gamsung_curation = gamsung_curation.append(pd.Series(data=[content,date,like,place,tags,imgUrl,name],index=gamsung_curation.columns),ignore_index=True)\n",
    "\n",
    "    gc_tmp = []\n",
    "    for i in gamsung_curation.name:\n",
    "        gc_tmp.append(gc_cnt[i]-1)\n",
    "\n",
    "    gamsung_curation['overlap'] = gc_tmp\n",
    "\n",
    "    #####################################################\n",
    "    ### hi.stay.tour.csv (ì™„ë£Œ)\n",
    "    hi_stay_tour_name = hi_stay_tour.content.apply(lambda x : hi_stay_tour_extract(x))\n",
    "    hi_stay_tour['name'] = hi_stay_tour_name\n",
    "    hi_stay_tour = hi_stay_tour[~(hi_stay_tour['name'].isna())]\n",
    "\n",
    "\n",
    "    # name dataê°€ ì—†ëŠ” index ì§€ìš°ê¸°\n",
    "    indexNames = hi_stay_tour[hi_stay_tour['name'] == ''].index\n",
    "    hi_stay_tour.drop(indexNames, inplace=True) \n",
    "\n",
    "\n",
    "    if hi_stay_tour.like.dtype == 'O':\n",
    "        hi_stay_tour.like = hi_stay_tour.like.apply(lambda x : convert(x))\n",
    "\n",
    "\n",
    "    # 1, ì¤‘ë³µëœ ì´ë¦„ ê²€ìƒ‰ \n",
    "    # 2.ê°œìˆ˜ê°€ 1ì¸ê±° ì´ìƒ ë½‘ì•„ë‚´ê¸° \n",
    "    # 3.ë¦¬ìŠ¤íŠ¸ë§Œë“¤ê¸°\n",
    "    hst_cnt = Counter(hi_stay_tour.name)      # Counter ëª¨ë“ˆë¡œ name data count\n",
    "    li =[]\n",
    "    for name, cnt  in hst_cnt.items():            # ì¤‘ë³µëœ ê²ƒë§Œ ë½‘ê¸° (cntê°€ 1ì´ìƒì´ë©´ ì¤‘ë³µ)\n",
    "        if cnt > 1:\n",
    "            li.append(name)  \n",
    "\n",
    "    # 4. í–‰ê°€ì ¸ì™€ì„œ likeì§‘ê³„, content, tag list append\n",
    "    for i in li:           # ì¤‘ë³µëœ name ë°˜ë³µ\n",
    "        ex = hi_stay_tour.loc[hi_stay_tour.name == i]  # ë°˜ë³µëœ nameì˜ í–‰ì„ ê°€ì ¸ì˜¨ë‹¤\n",
    "        ex.reset_index(drop=True,inplace=True)     # index reset\n",
    "\n",
    "        content = ex.iloc[0,:].content\n",
    "        date = ex.iloc[0,:].date\n",
    "        like = ex.iloc[0,:].like\n",
    "        place = ex.iloc[0,:].place\n",
    "        tags = ex.iloc[0,:].tags\n",
    "        imgUrl = ex.iloc[0,:].imgUrl\n",
    "        name = ex.name[0]\n",
    "\n",
    "        for i in range(len(ex)-1):\n",
    "            like += ex.iloc[i+1,:].like\n",
    "            tags += ex.iloc[i+1,:].tags\n",
    "\n",
    "\n",
    "\n",
    "        # 5.ì°¨ë¡€ëŒ€ë¡œ ê¸°ì¡´ í–‰ ì§€ì›Œë‚´ê¸°\n",
    "        indexNames = hi_stay_tour[hi_stay_tour['name'] == ex.name[0]].index\n",
    "        hi_stay_tour.drop(indexNames, inplace=True) \n",
    "        hi_stay_tour = hi_stay_tour.append(pd.Series(data=[content,date,like,place,tags,imgUrl,name],index=hi_stay_tour.columns),ignore_index=True)\n",
    "\n",
    "    hst_cnt_tmp = []\n",
    "    for i in hi_stay_tour.name:\n",
    "        hst_cnt_tmp.append(hst_cnt[i]-1)\n",
    "\n",
    "    hi_stay_tour['overlap'] = hst_cnt_tmp\n",
    "\n",
    "\n",
    "    #####################################################\n",
    "    ### rest_behappyhere.csv\n",
    "    rest_behappyhere_name = rest_behappyhere.content.apply(lambda x : rest_behappyhere_extract(x))\n",
    "    rest_behappyhere['name'] = rest_behappyhere_name\n",
    "    rest_behappyhere = rest_behappyhere[~(rest_behappyhere['name'].isna())]\n",
    "\n",
    "    # name dataê°€ ì—†ëŠ” index ì§€ìš°ê¸°\n",
    "    indexNames = rest_behappyhere[rest_behappyhere['name'] == ''].index\n",
    "    rest_behappyhere.drop(indexNames, inplace=True) \n",
    "\n",
    "\n",
    "\n",
    "    if rest_behappyhere.like.dtype == 'O':\n",
    "        rest_behappyhere.like = rest_behappyhere.like.apply(lambda x : convert(x))\n",
    "\n",
    "\n",
    "    # 1, ì¤‘ë³µëœ ì´ë¦„ ê²€ìƒ‰ \n",
    "    # 2.ê°œìˆ˜ê°€ 1ì¸ê±° ì´ìƒ ë½‘ì•„ë‚´ê¸° \n",
    "    # 3.ë¦¬ìŠ¤íŠ¸ë§Œë“¤ê¸°\n",
    "    rb_cnt = Counter(rest_behappyhere.name)      # Counter ëª¨ë“ˆë¡œ name data count\n",
    "    li =[]\n",
    "    for name, cnt  in rb_cnt.items():            # ì¤‘ë³µëœ ê²ƒë§Œ ë½‘ê¸° (cntê°€ 1ì´ìƒì´ë©´ ì¤‘ë³µ)\n",
    "        if cnt > 1:\n",
    "            li.append(name)\n",
    "\n",
    "\n",
    "    #####################################################\n",
    "    # 4. í–‰ê°€ì ¸ì™€ì„œ likeì§‘ê³„, content, tag list append\n",
    "    for i in li:           # ì¤‘ë³µëœ name ë°˜ë³µ\n",
    "        ex = rest_behappyhere.loc[rest_behappyhere.name == i]  # ë°˜ë³µëœ nameì˜ í–‰ì„ ê°€ì ¸ì˜¨ë‹¤\n",
    "        ex.reset_index(drop=True,inplace=True)     # index reset\n",
    "\n",
    "        content = ex.iloc[0,:].content\n",
    "        date = ex.iloc[0,:].date\n",
    "        like = ex.iloc[0,:].like\n",
    "        place = ex.iloc[0,:].place\n",
    "        tags = ex.iloc[0,:].tags\n",
    "        imgUrl = ex.iloc[0,:].imgUrl\n",
    "        name = ex.name[0]\n",
    "\n",
    "        for i in range(len(ex)-1):\n",
    "            like += ex.iloc[i+1,:].like\n",
    "            tags += ex.iloc[i+1,:].tags\n",
    "\n",
    "\n",
    "\n",
    "        # 5.ì°¨ë¡€ëŒ€ë¡œ ê¸°ì¡´ í–‰ ì§€ì›Œë‚´ê¸°\n",
    "        indexNames = rest_behappyhere[rest_behappyhere['name'] == ex.name[0]].index\n",
    "        rest_behappyhere.drop(indexNames, inplace=True) \n",
    "        rest_behappyhere = rest_behappyhere.append(pd.Series(data=[content,date,like,place,tags,imgUrl,name],index=rest_behappyhere.columns),ignore_index=True)\n",
    "\n",
    "    rb_cnt_tmp = []\n",
    "    for i in rest_behappyhere.name:\n",
    "        rb_cnt_tmp.append(rb_cnt[i]-1)\n",
    "\n",
    "    rest_behappyhere['overlap'] = rb_cnt_tmp\n",
    "\n",
    "    #####################################################\n",
    "    ### sookso.diary(ì™„ë£Œ)\n",
    "    sookso_diary_name = sookso_diary.content.apply(lambda x : sookso_diary_extract(x))\n",
    "    sookso_diary['name'] = sookso_diary_name\n",
    "    sookso_diary = sookso_diary[~(sookso_diary['name'].isna())]\n",
    "\n",
    "\n",
    "    # name dataê°€ ì—†ëŠ” index ì§€ìš°ê¸°\n",
    "    indexNames = sookso_diary[sookso_diary['name'] == ''].index\n",
    "    sookso_diary.drop(indexNames, inplace=True) \n",
    "\n",
    "\n",
    "\n",
    "    if sookso_diary.like.dtype == 'O':\n",
    "        sookso_diary.like = sookso_diary.like.apply(lambda x : convert(x))\n",
    "\n",
    "\n",
    "    # 1, ì¤‘ë³µëœ ì´ë¦„ ê²€ìƒ‰ \n",
    "    # 2.ê°œìˆ˜ê°€ 1ì¸ê±° ì´ìƒ ë½‘ì•„ë‚´ê¸° \n",
    "    # 3.ë¦¬ìŠ¤íŠ¸ë§Œë“¤ê¸°\n",
    "    sd_cnt = Counter(sookso_diary.name)      # Counter ëª¨ë“ˆë¡œ name data count\n",
    "    li =[]\n",
    "    for name, cnt  in sd_cnt.items():            # ì¤‘ë³µëœ ê²ƒë§Œ ë½‘ê¸° (cntê°€ 1ì´ìƒì´ë©´ ì¤‘ë³µ)\n",
    "        if cnt > 1:\n",
    "            li.append(name)\n",
    "\n",
    "    # 4. í–‰ê°€ì ¸ì™€ì„œ likeì§‘ê³„, content, tag list append\n",
    "    for i in li:           # ì¤‘ë³µëœ name ë°˜ë³µ\n",
    "        ex = sookso_diary.loc[sookso_diary.name == i]  # ë°˜ë³µëœ nameì˜ í–‰ì„ ê°€ì ¸ì˜¨ë‹¤\n",
    "        ex.reset_index(drop=True,inplace=True)     # index reset\n",
    "\n",
    "        content = ex.iloc[0,:].content\n",
    "        date = ex.iloc[0,:].date\n",
    "        like = ex.iloc[0,:].like\n",
    "        place = ex.iloc[0,:].place\n",
    "        tags = ex.iloc[0,:].tags\n",
    "        imgUrl = ex.iloc[0,:].imgUrl\n",
    "        name = ex.name[0]\n",
    "\n",
    "        for i in range(len(ex)-1):\n",
    "            like += ex.iloc[i+1,:].like\n",
    "            tags += ex.iloc[i+1,:].tags\n",
    "\n",
    "\n",
    "\n",
    "        # 5.ì°¨ë¡€ëŒ€ë¡œ ê¸°ì¡´ í–‰ ì§€ì›Œë‚´ê¸°\n",
    "        indexNames = sookso_diary[sookso_diary['name'] == ex.name[0]].index\n",
    "        sookso_diary.drop(indexNames, inplace=True) \n",
    "        sookso_diary = sookso_diary.append(pd.Series(data=[content,date,like,place,tags,imgUrl,name],index=sookso_diary.columns),ignore_index=True)\n",
    "\n",
    "    sd_cnt_tmp = []\n",
    "    for i in sookso_diary.name:\n",
    "        sd_cnt_tmp.append(sd_cnt[i]-1)\n",
    "\n",
    "    sookso_diary['overlap'] = sd_cnt_tmp\n",
    "\n",
    "    #####################################################\n",
    "\n",
    "    ### sookso.hada (ì™„ë£Œ)\n",
    "    sookso_hada_name = sookso_hada.content.apply(lambda x : sookso_hada_extract(x))\n",
    "    sookso_hada['name'] = sookso_hada_name\n",
    "    sookso_hada = sookso_hada[~(sookso_hada['name'].isna())]\n",
    "\n",
    "    # name dataê°€ ì—†ëŠ” index ì§€ìš°ê¸°\n",
    "    indexNames = sookso_hada[sookso_hada['name'] == ''].index\n",
    "    sookso_hada.drop(indexNames, inplace=True) \n",
    "\n",
    "\n",
    "    # like integet ë³€í™˜ í›„ í•©ì¹˜ê¸°\n",
    "\n",
    "    if sookso_hada.like.dtype == 'O':\n",
    "        sookso_hada.like = sookso_hada.like.apply(lambda x : convert(x))\n",
    "\n",
    "\n",
    "    # 1, ì¤‘ë³µëœ ì´ë¦„ ê²€ìƒ‰ \n",
    "    # 2.ê°œìˆ˜ê°€ 1ì¸ê±° ì´ìƒ ë½‘ì•„ë‚´ê¸° \n",
    "    # 3.ë¦¬ìŠ¤íŠ¸ë§Œë“¤ê¸°\n",
    "    sh_cnt = Counter(sookso_hada.name)      # Counter ëª¨ë“ˆë¡œ name data count\n",
    "    li =[]\n",
    "    for name, cnt  in sh_cnt.items():            # ì¤‘ë³µëœ ê²ƒë§Œ ë½‘ê¸° (cntê°€ 1ì´ìƒì´ë©´ ì¤‘ë³µ)\n",
    "        if cnt > 1:\n",
    "            li.append(name)        \n",
    "\n",
    "\n",
    "    # 4. í–‰ê°€ì ¸ì™€ì„œ likeì§‘ê³„, content, tag list append\n",
    "    for i in li:           # ì¤‘ë³µëœ name ë°˜ë³µ\n",
    "        ex = sookso_hada.loc[sookso_hada.name == i]  # ë°˜ë³µëœ nameì˜ í–‰ì„ ê°€ì ¸ì˜¨ë‹¤\n",
    "        ex.reset_index(drop=True,inplace=True)     # index reset\n",
    "\n",
    "        content = ex.iloc[0,:].content\n",
    "        date = ex.iloc[0,:].date\n",
    "        like = ex.iloc[0,:].like\n",
    "        place = ex.iloc[0,:].place\n",
    "        tags = ex.iloc[0,:].tags\n",
    "        imgUrl = ex.iloc[0,:].imgUrl\n",
    "        name = ex.name[0]\n",
    "\n",
    "        for i in range(len(ex)-1):\n",
    "            like += ex.iloc[i+1,:].like\n",
    "            tags += ex.iloc[i+1,:].tags\n",
    "\n",
    "        # 5.ì°¨ë¡€ëŒ€ë¡œ ê¸°ì¡´ í–‰ ì§€ì›Œë‚´ê¸°\n",
    "        indexNames = sookso_hada[sookso_hada['name'] == ex.name[0]].index\n",
    "        sookso_hada.drop(indexNames, inplace=True) \n",
    "        sookso_hada = sookso_hada.append(pd.Series(data=[content,date,like,place,tags,imgUrl,name],index=sookso_hada.columns),ignore_index=True)\n",
    "\n",
    "    sh_cnt_tmp = []\n",
    "    for i in sookso_hada.name:\n",
    "        sh_cnt_tmp.append(sh_cnt[i]-1)\n",
    "\n",
    "    sookso_hada['overlap'] = sh_cnt_tmp\n",
    "\n",
    "    ######################################\n",
    "    ##  ë°ì´í„° í”„ë ˆì„ í•©ì¹˜ê¸°\n",
    "    tot_dataset = daily_gamsung\n",
    "    for df in [gamsung_bnb, gamsung_curation, hi_stay_tour, rest_behappyhere, sookso_diary, sookso_hada]:\n",
    "        tot_dataset = tot_dataset.append(df)\n",
    "\n",
    "    if tot_dataset.like.dtype == 'O':\n",
    "        tot_dataset.like = tot_dataset.like.apply(lambda x : convert(x))\n",
    "\n",
    "\n",
    "\n",
    "    dg_cnt = Counter(tot_dataset.name)      # Counter ëª¨ë“ˆë¡œ name data count\n",
    "    li =[]\n",
    "    for name, cnt  in dg_cnt.items():            # ì¤‘ë³µëœ ê²ƒë§Œ ë½‘ê¸° (cntê°€ 1ì´ìƒì´ë©´ ì¤‘ë³µ)\n",
    "        if cnt > 1:\n",
    "            li.append(name)\n",
    "\n",
    "\n",
    "    # 4. í–‰ê°€ì ¸ì™€ì„œ likeì§‘ê³„, content, tag list append\n",
    "    for i in li:           # ì¤‘ë³µëœ name ë°˜ë³µ\n",
    "        ex = tot_dataset.loc[tot_dataset.name == i]  # ë°˜ë³µëœ nameì˜ í–‰ì„ ê°€ì ¸ì˜¨ë‹¤\n",
    "        ex.reset_index(drop=True,inplace=True)     # index reset\n",
    "\n",
    "        content = ex.iloc[0,:].content\n",
    "        date = ex.iloc[0,:].date\n",
    "        like = ex.iloc[0,:].like\n",
    "        place = ex.iloc[0,:].place\n",
    "        tags = ex.iloc[0,:].tags\n",
    "        imgUrl = ex.iloc[0,:].imgUrl\n",
    "        name = ex.name[0]\n",
    "        overlap = ex.iloc[0,:].overlap\n",
    "\n",
    "        for i in range(len(ex)-1):\n",
    "            like += ex.iloc[i+1,:].like\n",
    "            tags += ex.iloc[i+1,:].tags\n",
    "            overlap += ex.iloc[i+1,:].overlap\n",
    "\n",
    "\n",
    "        # 5.ì°¨ë¡€ëŒ€ë¡œ ê¸°ì¡´ í–‰ ì§€ì›Œë‚´ê¸°\n",
    "        indexNames = tot_dataset[tot_dataset['name'] == ex.name[0]].index\n",
    "        tot_dataset.drop(indexNames, inplace=True) \n",
    "        tot_dataset = tot_dataset.append(pd.Series(data=[content,date,like,place,tags,imgUrl,name,overlap],index=tot_dataset.columns),ignore_index=True)\n",
    "\n",
    "    ####################################\n",
    "    ## Naver ì¥ì†Œ API\n",
    "\n",
    "    # naver application clientID,secret\n",
    "    client_id = \"6E6mBjvxbdEgVk6Prqgd\"\n",
    "    client_secret = \"0ZjWnYiBjr\"\n",
    "\n",
    "\n",
    "    # quoteê°€ ì¸ì½”ë”©ì„ ë³€ê²½(UTF8ë¡œ urlì— ë„˜ê²¨ì£¼ê¸° ìœ„í•´ì„œ)\n",
    "    # ë‚´ê°€ ì…ë ¥í•œ ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ parse\n",
    "    title_et = []\n",
    "    for name in tot_dataset.name:\n",
    "        query = urllib.parse.quote(f'ìˆ™ì†Œ+\" \"+{name}')\n",
    "        idx = 0 # ìš”ì²­ ê°œìˆ˜\n",
    "        display = 10 # 100ê°œ ë‹¨ìœ„ë¡œ ê°€ì ¸ì˜¨ë‹¤\n",
    "        start = 1\n",
    "        end = 100\n",
    "        tmp=''\n",
    "\n",
    "        url = \"https://openapi.naver.com/v1/search/local?query=\" + query \\\n",
    "            + \"&display=\" + str(display) \\\n",
    "            + \"&start=\" + str(start)\n",
    "\n",
    "\n",
    "        request = urllib.request.Request(url)\n",
    "        request.add_header(\"X-Naver-Client-Id\",client_id)\n",
    "        request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
    "        # request ìš”ì²­ì— ëŒ€í•œ ê²°ê³¼ë¥¼ responseì— ì €ì¥\n",
    "        response = urllib.request.urlopen(request)\n",
    "        rescode = response.getcode()\n",
    "        if(rescode==200):\n",
    "            response_body = response.read()\n",
    "            response_dict = json.loads(response_body.decode('utf-8'))\n",
    "            items = response_dict['items']\n",
    "            for item_index in range(0,len(items)):\n",
    "                remove_tag = re.compile('<.*?>')\n",
    "                title = re.sub(remove_tag, '', items[item_index]['title'])\n",
    "                link = items[item_index]['link']\n",
    "                caterory = items[item_index]['category']\n",
    "                description = re.sub(remove_tag, '',items[item_index]['description'])\n",
    "                telephone = items[item_index]['telephone']\n",
    "                address = items[item_index]['address']\n",
    "                roadAddress = items[item_index]['roadAddress']\n",
    "                mapx = items[item_index]['mapx']\n",
    "                mapy = items[item_index]['mapy']\n",
    "\n",
    "                # ì´ë¦„ì´ ì—†ëŠ” ê²½ìš°\n",
    "                if not title :\n",
    "                    title_et.append('')\n",
    "                else: #'ì „í†µìˆ™ì†Œ' 'í˜¸ìŠ¤í…”''ë¦¬ì¡°íŠ¸''ë¦¬ì¡°íŠ¸ë¶€ì†ê±´ë¬¼'\n",
    "                    if caterory[:2] in ['ìˆ™ë°•'] :  # ì¹´í…Œê³ ë¦¬ê°€ ìˆ™ë°•ì¸ ê²½ìš°\n",
    "                        title = title.replace(' ','')\n",
    "                        name = name.replace(' ','')\n",
    "                        ratio = SequenceMatcher(None, title, name).ratio()\n",
    "                        if ratio >= 0.7 : # title.replace(' ','') in name.replace(' ','')\n",
    "                            tmp = address\n",
    "\n",
    "            if tmp : title_et.append(tmp)\n",
    "            else: title_et.append('')\n",
    "\n",
    "        else:\n",
    "            print(\"Error Code:\" + rescode)\n",
    "\n",
    "        time.sleep(0.1)\n",
    "\n",
    "\n",
    "    tot_dataset.drop('place',axis=1,inplace=True)\n",
    "    tot_dataset['place'] = title_et\n",
    "\n",
    "    #####################################\n",
    "    ### ì½”ë¡œë‚˜ ë‹¨ê³„ ì‘ì—…\n",
    "    coronaStage = coronaStage.toPandas() \n",
    "\n",
    "    stage =[str() for i in range(len(tot_dataset))]\n",
    "    local = [str() for i in range(len(tot_dataset))]\n",
    "    for lo in coronaStage.Area.unique().tolist():\n",
    "        for j in range(len(tot_dataset)):\n",
    "            try :\n",
    "                com = tot_dataset.iloc[j].place.split()[0]\n",
    "                if list(lo)[0] in com and list(lo)[1] in com:\n",
    "                    stage[j] = coronaStage[coronaStage.Area == lo].Stage.values[0]\n",
    "                    local[j] = lo\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    tot_dataset['stage'] = stage\n",
    "    tot_dataset['local'] = local\n",
    "\n",
    "    ############################\n",
    "    ## ìƒˆë¡œìš´ ìˆ™ì†Œ ìƒì„±\n",
    "\n",
    "\n",
    "    ch = []\n",
    "    for i in cur.find():\n",
    "        ch.append(i)\n",
    "    change = pd.DataFrame(ch)\n",
    "\n",
    "    a = tot_dataset.merge(change,how='left' ,on='name')\n",
    "    li = a[a.tags_y.isnull() == True].index.tolist()\n",
    "    new = tot_dataset.iloc[li]\n",
    "    print(new)\n",
    "#     new.drop('index',axis=1,inplace=True)\n",
    "    new.to_parquet(f'new_{nowtime}.parquet')\n",
    "\n",
    "insta_spark_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "breathing-slovenia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "applicable-myanmar",
   "metadata": {},
   "outputs": [],
   "source": [
    "change['imgUrl'] = a['imgUrl_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "brief-chapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "change.drop('index',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "electrical-maryland",
   "metadata": {},
   "outputs": [],
   "source": [
    "change.to_parquet(f'tot_dataset_{nowtime}.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-placement",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honest-purple",
   "metadata": {},
   "source": [
    "### í´ëŸ¬ìŠ¤í„°ë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a5c4e69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "insta_data = tot_dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "genetic-analyst",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:4: DeprecationWarning: `prepare()` has no effect and will be removed in future version.\n"
     ]
    }
   ],
   "source": [
    "from kiwipiepy import Kiwi\n",
    "\n",
    "kiwi = Kiwi()\n",
    "kiwi.prepare()\n",
    "\n",
    "location = pd.read_csv('/home/ubuntu/DS/location_words.csv')\n",
    "location.columns = ['index', 'location']\n",
    "location = location.location.tolist()\n",
    "\n",
    "def sub_special(s):\n",
    "    rs = re.sub(r'[^ê°€-í£]',' ',s)\n",
    "    rr = re.sub(' +', ' ', rs)\n",
    "    return rr\n",
    "\n",
    "def tokenize_nouns(sent):\n",
    "    res, score = kiwi.analyze(sent)[0] # ì²«ë²ˆì§¸ ê²°ê³¼ë¥¼ ì‚¬ìš©\n",
    "    return [word + ('ë‹¤' if tag.startswith('V') else '') # ë™ì‚¬ì—ëŠ” 'ë‹¤'ë¥¼ ë¶™ì—¬ì¤Œ\n",
    "            for word, tag, _, _ in res\n",
    "            if tag.startswith('N')] # ì¡°ì‚¬, ì–´ë¯¸, íŠ¹ìˆ˜ê¸°í˜¸ ë° stopwordsì— í¬í•¨ëœ ë‹¨ì–´ëŠ” ì œê±°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5b1ff8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n",
      "/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# 1ì°¨ í´ë¦°ì§•(ì˜ì–´, íŠ¹ìˆ˜ë¬¸ì, ìˆ«ìì œê±°)\n",
    "for i in range(len(insta_data.content)):\n",
    "    insta_data.content[i] = sub_special(insta_data.content[i])\n",
    "\n",
    "# í† í°í™”\n",
    "for i in range(len(insta_data.content)):\n",
    "    insta_data.content[i]= tokenize_nouns(insta_data.content[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d23ba53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "insta_data['location'] = np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b1bd1693",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = 'ê°ì„±,ì¸,ìˆ˜,ë“±,ë‚˜,ì• ì •,ìƒí™©,ë¬¸ì˜,ì‘ë™,ê¸ˆ,ë™,ì†,ì²œì¥,ì„ì–‘,í•´,í•´ì•ˆ,ë‚¨,ì¶”ì²œ,ë°©ì†¡,ìš°ì •,ì†Œì›,ë™ì¼,ë³µ,ìˆ˜,ë‚´,ì•ˆì „,ì„,ê°€ëŠ¥,ê°€ì¹˜,ë™ë¦¬,íšŒ,ì‚¬ë°©,ê¶,ë‹¹,ëŒ€,ë™ë§‰,ì‚¬ì§„,í†µ,ì°½,ê°,êµ¬ì„±,ì ,ì‹œ,ì „,ì¡°ë¦¬,ìš”ë¦¬,ì¡°ë¦¬,ë…¸ì²œ,ì˜¨ìˆ˜,ì˜ì§€,ê¸¸,ì¥,í›„,ì…,ì‹œ,ì›,ê°€êµ¬,ë‚´ë¶€,ì˜¤'\n",
    "stop_words = stop_words.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6ce0ea73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(insta_data.content)):\n",
    "    result = []\n",
    "    for w in insta_data.content[i]:\n",
    "        if w in location and w not in stop_words:\n",
    "            result.append(w)\n",
    "        insta_data.location[i] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "47874fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(insta_data.location)):\n",
    "    temp = set(insta_data.location[i])\n",
    "    insta_data.location[i] = list(temp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8bb61e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          [ë°€ì–‘, ë°€ì–‘ì‹œ, ì‚°ì™¸ë©´]\n",
       "1                    [ì—¬ìˆ˜]\n",
       "2      [í™”ë„ë©´, ê°•í™”êµ°, ê°•í™”, ì¸ì²œ]\n",
       "3          [ì œì£¼ì‹œ, êµ¬ì¢Œì, ì œì£¼]\n",
       "4           [ì „ë¶, ì „ì£¼, ì „ì£¼ì‹œ]\n",
       "              ...        \n",
       "694    [ìƒê´€ë©´, ì™„ì£¼, ì™„ì£¼êµ°, ì „ì£¼]\n",
       "695        [ê°•ë¦‰ì‹œ, ê°•ë¦‰, ì—°ê³¡ë©´]\n",
       "696    [ì¤‘ì•™, ê°•ë¦‰ì‹œ, ê°•ë¦‰, ì‚¬ì²œë©´]\n",
       "697        [í¬í•­, í¬í•­ì‹œ, ì²­í•˜ë©´]\n",
       "698         [ì •ë°©, ì„œê·€í¬, ì œì£¼]\n",
       "Name: location, Length: 699, dtype: object"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insta_data.location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fea4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "# pymongo connect\n",
    "client = MongoClient('localhost',27017) # mongodb 27017 port\n",
    "db = client.ojo_db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3995c110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # mognodbì— ë„£ê¸°ìœ„í•œ json í˜•íƒœ ë³€í™˜ í›„ mongodb collection(corona)ë¡œ ì ì¬\n",
    "# for k in range(len(tot_dataset)):\n",
    "#     tmp ={}\n",
    "#     for key,value in zip(tot_dataset.columns,tot_dataset.values[k]):\n",
    "#         tmp[key] = value\n",
    "#     db.insta_tot.insert_one(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2395f41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_python3)",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
